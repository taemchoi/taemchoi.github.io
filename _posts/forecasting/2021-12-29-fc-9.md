---
title: "TFT : Temporal FUsion Transformers for interpretable multi-horizon"
tags: [AI, Timeseries, 코드 구현, Forecasting]
categories:
  - Forecasting
date: 2021-12-29
toc : true
---

- [Abstract](#0-abstract)
- [Introduction](#1-introduction)
- [Related work](#2-related-work)
- [Multi-horizon forecasting](#3-multi-horizon-forecasting)
- [Model Architecture](#4-model-architecture)
  - [Gating mechanisms](#41-gating-mechanisms)
  - [Variable selection networks](#42-variable-selection-networks)
  - [Static covariate encoders](#43-static-covariate-encoders)
  - [Interpretable multi-head attention](#44-interpretable-multi-head-attention)
  - [Temporal fusion decoder](#45-temporal-fusion-decoder)
  - [Quantile outputs](#46-quantile-outputs)
- [Loss function](#5-loss-functions)
- [Performance evaluation](#6-performance-evaluation)
- [Interpretability use cases](#7-interpretability-use-cases)
- [Conclusions](#8-conclusions)

Multi-horizion forecasting이란, 단일 step을 예측하는 것이 아닌 여러 미래의 step을 예측하는 것

## 0. Abstract
- Multi-horizion forecasting은 복잡성이 혼합된 input ~(과거에에 관측된 타겟과 어떻게 연관되어있는지 사전 정보가 없는 다른 외인성(exogenous) 시계열과 미래의 input으로 알려진 정적 공변량)~ 을 종종 가지고 있다.
- 여러 딥러닝 method들이 제안되었지만 전형적으로 그것들은 실제 시나리오에서 현재 전체 길이의 인풋이 어떻게 사용되었는지 알 수 없는 'black-box' 모델이다.
- 그래서 해당 논문에서는 해석가능한 인사이트와 함게 고성능 multi-horizon forcasting을 할 수 있는 새로운 attention 기반의 mechanism을 갖고 있는 **TFT(Temporal Fusion Transformer)**를 소개한다.

## 1. Introduction
- Multi-horizon forecating은 시계열 기계학습에서 중요한 문제이다.
- 한 step만 예측하는 것과 달리, 사용자에게 전체 시계열에 대한 추정치를 접근가능하게 하고 이를 토대로, 그들이 원하는 미래의 여러 step에 대한 최적화된 행동을 할 수 있게 한다.
- 실제로 Multi-horizon forecasting은 retail, health-care, economics 등 실제 세계에 많은 영향을 끼치고 이런 method들의 성능향상은 매우 가치 있는 일이다.
- 실제 Multi-horizon forecasting은 일반적으로 서로 어떻게 상호작용하는지 모르는 다양한 데이터 원천(*미래에 대한 정보(휴일 등), 외인성 timeseries(유동인구 기록 등)정적 metadata(상점 위치 등)*)에 접근하게 된다.
- 상호작용에 대한 미약한 정보를 알고 있는 이런 데이터 원천들에 대한 이질성(heterogineity)은 Multi-horizon forecasting을 어렵게 한다.
- DNN은 이의 시계열 모델에 비해 강한 성능을 증명하고 있다. 그런데 많은 아키텍처들은 RNN 아케텍처에 초점을 맞추고 있고 최근 관련된 time step의 선택을 강화한 attention 기반의 method도 사용으로 개선되고 있다.
- 그런데 이것들은 외인성 input을 미래에 알 수 있다는 가정(자기회귀 모델의 일반적인 문제)이나 정적인 공변량을 무시한 가정(다른  time-dependent feature과 각 time step에서 결합) 등 흔히 있는 다른 type의 인풋들에 대한 도입 고려가 종종 실패하게 된다.
- 시계열 모델의 최근 발전은 데이터 고유한 특성을 갖고 있는 구조를 정렬함으로써 성과를 내었다.
- 우리는 적절한 귀납적 bias를 가진 모델을 설계함으로써 성과를 주장하고 증명하였다. 
- input의 이질성을 고려하지 않은 대부분의 현재 모델들은 파라미터간 비선형성 상호작용에 의해 예측하는 Blackbox 모델이다.
- 이는 모델이 어떻게 결과물을 가져왔는지 어렵게 하고 유저들이 결과를 신뢰하거나 개발자가 디버깅하는데도 어려움을 준다.
(설명가능한 DNN은 시계열에 적합하지 않다. - post-hoc 기법은 time step을 고려하지 않고 특징만 봄)
- **TFT**는 Attention 기반의 해석력을 제공하고 전체 구간의 잠재적인 인풋들의 시간관계를 가진 구조를 정렬하기 위해 아래의 4가지 idea를 제안하였다.
    1. **Static covariate encoder**(input context를 다른 network에서 사용하기 위해 encoding함)
    2. **Gating mechanism**(관계 없는 input들의 기여를 최소하는 전체와 표본의 종속 변수선택) 
    3. **Seq2seq layer**(알고 있는 관측된 input을 지역적으로 처리)
    4. **A temporal self attention decoder**(dataset의 long term dependency를 학습)

## 2. Related work
### 2.1. DNNs for Multi-horizon Forecasting
- 전통적 방법과 유사하게, 최근 딥러닝도 Autoregressive model를  사용한 iterated(반복되는) 접근과 seq2seq를 토대로한 directed method로 분류된다.
    1. **Iterated methods**
        - 한 스텝 예측을 반복해서 multi step의 예측값을 얻는다.(결과를 미래의 input으로 활용)
        - LSTM네트워크가 고려되는데 DEEPAR(가우시안 예측 분포의 파라마터를 생성하기 위해 LSTM LAYER를 쌓음), DSSM(Kalman filtering을 통해 생성된 예측 분포와 사전정의된 선형상태공간의 파라미터를 생성하기 위해 LSTM 활용) 등이 있다.
        - 최근에는 transformer 기반의 구조들이 예측시 local processing을 하기 위한 convolution layer와 예측 동안 receptive field를 늘리기 위한 sparse attenttion mechanism이 제시되었다.
        - 이런 단순함에도 불구하고 반복 기법은 타겟을 제외한 모든 변수는 미래값을 알 수 있다는 가정을 하고 있다.
        - 그러나, 많은 실제 시나리오 에서는 미래값을 알 수 없는 거대한 사용가능한 여러 시간이 존재한고 그렇기 때문에 실제 사용에는 제약이 따른다.
        - 반면, **TFT**는 입력의 다양성을 설명할 수 있다.
    2. **Direct methods**
        - 각 시간 단계에서 사전정의된 multiple horizon을 예측을 한번에 생성하도록 훈련된다.
        - 이런 구조는 전형적으로 seq2seq 모델에 의존한다.(과거를 요약하는 LSTM인코더와 미래를 생성하기 위한 다양한 methods)
        - MQRNN(Multi-horizon Quantile Recurrent forecaster)은 LSTM이나 conv 인코더를 이용하여 context vector를 만들고 이를 각각의 horizon에서 Multi layers perceptron에 feed하여 예측한다.
        - Multi-modal attention mechanism은 context vector를 만드는 LSTM encoder와 함께 Bi-directional LSTM의 디코더를 사용한다.
        - 1.의 반복 method에 비해 나은 성능에도 불구하고 여전히 설명력에 대한 문제가 남아있다.

### 2.2. Time Series Interpretability with Attention
- attention mechanism은 어텐션 가중치를 사용하여 각 instance에서 입력의 핵심적인 부분을 식별하기 위해 번역, 이미지 분류, tabular learning에서 많이 사용된다.
- 최근 LSTM과 Transformer의 구조를 기반으로한 attention mechanism은 시계열의 설명력을 위해 사용되었다.
- 그런데 static covarites를 고려하지 않고 사용하였다.
- TFT는 이러한 문제를 static feature들에 대하여 개별적인 encoder-decoder attention을 적용하여 해결하고자 한다.

### 2.3. Instance-wise Variable Importance with DNNs
- 변수의 중요도는 post-hoc method(LIME, SHAP, RL-LIM → 시계열을 설명하도록 설계되지 않음)와 inherently interpretable models(Interpretable Multi-variable LSTMs → global 시계열에 대한 설명이 아닌 sample에 대한 설명만 가능, 한 step에 대한 설명 가능)로 얻을 수 있다.
- TFT는 Global 시간 관계와 user가 dataset에 대한 전체 시간관계를 해석할 수 있게 해주었다.

## 3. Multi-horizon forecasting
- 주어진 시계열 데이터셋에 고유한 entity I가 있다고 하자.(retail에서의 다른 상점들이나 healthcare에서의 환자들)
- 각 entity i는 각 time step t([ 0,T<sub>i</sub> ])에서 정적 공변량 s<sub>i</sub> 또는 입력 x<sub>i, t</sub> 그리고 scaler, 타겟 y<sub>i,t</sub>로 구성되어 있다.
- 시간 종속 input feature들은 2가지 카테고리로 세분화된다. x<sub>i,t</sub> = [ z<sup>T</sup><sub>i,t</sub>, x<sup>T</sup><sub>i,t</sub> ]
    1. z<sub>i,t</sub> : 각 단계에서 오직 측정될 수 있는 알려진 input
    2. x<sub>i,t</sub> : 미리 결정될 수 있는 알려진 input
- 많은 시나리오에서 예측 간격을 제공하는 것은 target으로 가질 수 있는 best와 worst 케이스 같은 것을 산출함으로써 결정 최적화와 리스크 관리에 유용할 수 있다.  
- 그래서 Quantile regression을 적용하였고 수식은 아래와 같다.
<img src="/img/fc/fc9/fc9-0.jpg">

- y<sub>i,t+τ</sub>(q,t,τ) 는 quantile q에서 t시간에 τstep 앞선 예측 값이다. 그리고 f<sub>q</sub>는 예측 모델이고 k는 look-back window로 t부터 k개 이전의 값을 사용한다.  

## 4. Model architecture
<p align="center" style ="font-size:small">
<img src="/img/fc/fc9/fc9-1.jpg">
TFT는 정적인 metadata, past input과 known feature input이 시차를 두고 있다.
변수 선택은 input을 기반으로한 가장 두드러진 feature 선택의 판단에 사용됩니다. GRN Block은 skip connection과 gating layer들로 효과적인 정보 전달을 가능하게합니다. 시간 종속 처리과정은 local processing을 위한 LSTM계열과 각 time step의 정보들을 통합하기 위해 multi-head attention을 사용합니다.
</p>

- TFT는 넓은 범위의 문제들에서 각 input type들에 대해 높은 예측 성능으로 효과적으로 특징 표현을 설계하기 위해  
 표준 구성요소를 사용하여 디자인되었다.
  1. Gating mechanisim : 사용되지 않은 성분은 skip over 하고, 넓은 범위의 데이터셋과 시나리오들을 수용하기위한 adaptive depth와 네트워크 복잡성을 제공한다. 
  2. Variable selection entworks : 각 time step에서 관련된 input 변수들을 선택한다.
  3. Static covariate encoders :  네트워크에서 context vector 인코딩을 통해서 정적인 feature들을 통합한다. 
  4. Temporal processing : 관측되고 알고있는 다양한 시간의 input들로부터 단·장기 시간관계를 둘다 배운다. seq2seq layers 들은 local processing을 처리하고 장기 의존성은 새로운 설명가능한 multi-head attention을 통해 학습한다.
  5. Prediction intervals : quantile forecast를 통해 target의 예측 구간을 생성한다.

### 4.1. Gating mechanisim
<img src="/img/fc/fc9/fc9-1-1.jpg">
- 외인성 input과 target 간의 정확한 관계는 종종 미리 알 수 없기 때문에 어떤 변수가 관련이 잇는 지 예상하기 어렵다. 게다가, 얻어진 비선형 처리의 확장을 결정하는 것을 어렵게 하고 데이터셋이 작거나 noisy가 있을 때와 같이 작은 데이터셋이 효과적인 경우가 있기도 하다.
- 오직 필요한 곳에서 비선형 처리를 적용하여 주어진 모델의 유연성을 높이고자 **Gate Residual Network(GRN)**을ㅇ 제안하고 있다.
- GRN은 기존 input a와 보조적 context vector c를 받아 아래와 같이 처리한다.
<img src="/img/fc/fc9/fc9-2.jpg"> ELU(Exponetial Linear Unit activation function), η<sub>1</sub>과 η<sub>2</sub>는 중간층이고, Layer Norm은 일반적인 layer normalization이다. w의 index는 가중치 공유를 나타낸다.

- 쓸모없는 것은 숨기는 유연성을 제공하기 위해 Gated Linear Units(GLUs)를 베이스로한 Gating Layer를 구성하였다.
- GLUs는 다음과 같다. 
<img src="/img/fc/fc9/fc9-3.jpg">

- σ는 활성함수 sigmoid이다. 
- GLU는 TFT가 original input a가 GRN에서 기여하는 정도를 조절할 수 있게 한다. 필요한 경우 비선형의 기여도를 억압하기 위해 GLU의 OUTPUT을 0에 가깝게 만들면 잠재적으로 layer를 뛰어넘는 효과가 있다. context vector가 없을 때, GRN은 단순히 context 입력을 0이라 간주하게 된다.
- 훈련동안에는 gating layer와 layer Normalization 앞에 dropout이 적용되었다. 

### 4.2. Variable selection networks
<img src="/img/fc/fc9/fc9-1-2.jpg">

- 다양한 변수가 이용될 때, 그들의 관계와 구체적은 output에 대한 기여도는 본질적으로 알 수 없다.
- TFT는 정적공변량과 시간종속 공변량에 모두 적용된 변수 선택 네트워크를 사용을 통해 instance-wise variable selection을 제공하도록 설계되었다.
- 예측에 가장적합한 변수에 대한 insight를 제공하는 것을 넘어, 변수 선택은 TFT가 성능에 부정적 영향을 끼치는 불필요한 noisy input을 제거하는 역할도 한다.
- 실세계에서 시계열 데이터셋은 더 적은 predictive content를 포함하고 있다. 그러므로, 변수 선택은 가장 두드러진 것들을 학습에 활용하여 모델 성능 향상에 도움을 준다.
- 특징 표현으로서 <u>범주형 변수에 entity embedding을 연속형 변수는 선형변환을 사용</u>하였다.
→ skip connection을 위해 차원을 맞춰줌
- 모든 정적, 과거와 미래 input들은 별도의 변수선택 네트워크를 사용하였다. (구조 그림의 색깔로 구분)
- 일반화의 손실없이, 아래와 같이 과거 input에 대한 변수 선택을 표현하였다. j번째 변수의 시간 t에 대한 input을 ε<sup>(j)</sup><sub>t</sub>라고 하면, Ξ<sub>t</sub>는 t시점 이전의 모든 input들을 펼쳐놓은 벡터가 된다. [static covariate encoder](#43-static-covariate-encoders)로 부터 얻어진 external context vector c<sub>s</sub>와 함께 Ξ<sub>t</sub>는 GRN의 input으로 하여 softmax layer를 아래와 같이 지나게 된다.
<img src="/img/fc/fc9/fc9-4.jpg">

- 각 시점에서, 비선형 처리의 추가적인 layer가 자체 GRN을 통해 각각의 ε<sup>(j)</sup><sub>t</sub>를 제공함으로써 사용된다.
- 자체 GRN은 모든 시간에 대해 가중치를 공유한다.  
- 추정된 ε<sub>t</sub>는 아래와 같이 구한다.
<img src="/img/fc/fc9/fc9-5.jpg">

### 4.3. Static covariate encoders
- 다른 시계열 예측 구조와 다르게, TFT는 4가지 다른 context vector c<sub>s</sub>, c<sub>e</sub>, c<sub>c</sub>, c<sub>h</sub>를 생성하기 위한 별도의 GRN encoder들을 사용함으로써 정적 metadata로부터의 정보 통합을 하였다.
- 이 context vector들은 temporal fusion decoder(정적인 변수들이 중요한 역할을 하는 곳)에서 다양한 영역에 연결되어있다. 
-  c<sub>s</sub>(시간변수 선택), c<sub>e</sub>(정적 정보와 함께하는 시간적 특징), c<sub>c</sub>(local processing), c<sub>h</sub>(local processing)는 ()안의 의미를 갖고 있다. 
- ex) c<sub>s</sub> = GRN<sub>c<sub>s</sub></sub>(S)   →   구조그림에서 노란색 선 해당

### 4.4. Interpretable multi-head attention
- TFT는 서로 다른 시간의 장기 관계성을 학습하기 위해 기존 transformer의 구조에서 수정하여 설명력을 강화한 self-attetion mechanism을 활용하였다. 
- tranformer의 multi head-attention의 구조가 궁금하다면 [링크](https://taemchoi.github.io/nlp/nlp-1/#322-multi-head-attention)를 참조하면 된다.
<p align="center" style="font-size:small"><img src="/img/fc/fc9/fc9-6.jpg">기존 transformer의 multi-head attention 수식</p>

- transformer의 attention 구조는 각 헤드에 다른 값들이 주어져서, 특정 feature들의 중요성을 나타내는 지표가 되지 못한다.
- 그래서, Interpretable multi-head attention는 각 헤드들이 값을 공유하도록 수정하였고 모든 head에 addictive aggregation을 적용하였다.
<p align="center" style="font-size:small"><img src="/img/fc/fc9/fc9-7.jpg">설명력을 높인 multi-head attention 수식</p> 

- W<sub>v</sub>는 가중치값을 모든 헤드들과 공유하고 있다. 그리고 마지막 linear연산에 W<sub>h</sub>가 사용되어진다.
- 수식 (9)와 수식(14)를 비교해보면, 설명력을 높인 multi-head attention의 마지막 output은 하나의 single head attention과 매우 닮은 것을 알 수 있다.(attention weight를 생성하기 위한 중요한 방법론 차이)
- 수식 (15)로 부터, input feature V 집합이 들어오면, 각 헤드는 다른 시간의 패턴을 학습하게 된다. (이는 일종의 간단한 앙상블로 해석할 수 있다.)
- 수식 (14)의 A<sup>~</sup>는 단일 헤드의 attention weight들의 분석을 수행함으로써 간단하게 설명력을 학습하여 representation capacity를 효과적으로 증가시켰다.

### 4.5. Temporal fusion decoder
- **TFD(Temporal Fusion Decoder)**는 일련의 층들을 사용하여 아래에 설명된 데이터셋의 시간 관계를 학습한다.

#### 4.5.1. Locality enhancement with sequence-to-sequence layer
- 시계열 데이터에서, 중요한 포인트는 그 주변 값들에 대한 관계(시계열의 흐름)를 밝혀내는 것이다.(이상치, 변곡점, 순환 패턴 )
- top of point-wise value에 패턴 정보를 활용하는 기능의 구성을 통해 local context를 활용하면 attention 기반 아키텍처의 성능을 향상시킬 수 있다. ~~컨볼루션으로 특정 지점마다의 특징을 뽑아서 활용한다는 내요인 것 같은데 잘 모르겠다~~ (예를 들면, 지역성 강화를 위해 단일 컨볼루션 layer를 사용하는 것(모든 시간에 같은 필터를 적용해서 로컬 패턴을 추출))
- 그러나 과거와 미래 input의 수가 다르기 때문에 observed inputs이 존재할 경우는 어울리지 않을 수 있다.
→ `컨볼루션등을 사용해서 local context를 뽑아내는 것은 시계열에서는 적합하지 않음을 시사`
- 그러므로 TFT는 이 차이들을 자연스럽게 처리하기 위해 **sequence-to-sequence layer** 구조를 도입하였다. (인코더에는 ε<sub>t-k:t</sub>를 디코더에는 ε<sub>t+1:t+τ<sub>max</sub></sub>) 입력으로 준다.)
- 이것은 TFD 자체에 INPUT으로 사용되는 uniform한 시간 특징을 생성하게 된다.
- TFT는 비교를 위해 다른 mulit-horizon 구조와 동일하게 LSTM으로 구성된 encoder-decoder 구조를 갖고 있다.(시계열 input의 적절한 bias 지표를 제공하여 표준화된 positional encoding의 대체를 시사함)
- 게다가, static metadata가 local processing에 영향을 미칠수 있도록 우리는 c<sub>c</sub>, c<sub>h</sub>를 사용해서 cell state와 hidden state를 각각 초기화해주기 위해 사용하였다.
- 또한, gate skip-connection을 아래와 같이 적용하였다.
<p align="center" style="font-size:small"><img src="/img/fc/fc9/fc9-8.jpg">n은 [ -k, τ<sub>max</sub> ]</p> 

#### 4.5.2. Static enrichment layer
- 시간 흐름에 확실이 영향을 주는 정적 공변량이 있을 때, static metadata의 시간 특징을 강화하는 **static enrichment layer**를 사용한다.(예를 들면, 질병 위험에 대한 유전적 정보)
<img src="/img/fc/fc9/fc9-9.jpg">

#### 4.5.3. Temporal self-attetion layer
- [Static enrichment](#452-static-enrichment-layer) 다음으로, self attetion을 적용한다.
- 모든 시간적 특징이 강화된 정적 특징들은 첫번째로 단일 행렬로 그룹된다. 그리고 Interpretable multi-head attetion에 각 예측 시간에 적용된다.
<p align="center" style="font-size:small"><img src="/img/fc/fc9/fc9-10.jpg">Θ(t) = [ θ(t, -k), ..., θ(t, τ) ]<sup>T</sup>,   <b>Β</b>(t) = [ β(t,-k), ..., β(t, τ<sub>max</sub>) ],     d<sub>v</sub> = d<sub>attn</sub> = d<sub>model</sub>/m<sub>H</sub> </p>

- Decoder의 masking은 예측하는 시점 앞 시점의 데이터만 활용하도록 보장하기위해 multi-head attention에 적용되었다.
- 일반적인 정보 제한외에도, self-attention layer는 RNN 기반의 구조가 학습하기 어려운 장기의존성을 학습할 수 있습니다.
- 아래와같이, self-attention layer와 gating layer가 함께 훈련이 용이하도록 적용되어있다. 
<img src="/img/fc/fc9/fc9-11.jpg">

#### 4.5.4. Position-wise feed-forward layer
- self-attetion의 output에 비션형처리를 추가하였다.
- [Static enrichment](#452-static-enrichment-layer)와 마찬가지로 GRN을 사용한다.
- 그리고 [TFD](#45-temporal-fusion-decoder)를 지나치는 residual connection을 적용하였다.
<p align="center" style="font-size:small"><img src="/img/fc/fc9/fc9-12.jpg">그림의 빨간색 번호가 수식을 의미함 </p>

#### 4.6. Quantile outputs
- 점 예측에 구간 예측을 제공하는데, 이것은 각 time step에 다양한 상화을 부여할 수 있다.(10th, 50th, 90th 등)
- **Quantile foreacast**는 [TFD](#45-temporal-fusion-decoder)의 ouput에 선형 변환을 사용하여 예측값을 생성한다.
<img src="/img/fc/fc9/fc9-13.jpg">

## 5. Loss Function
- 모든 quantil output을 합한 Quantile loss 공통적으로 최소화하도록 학습한다.
<p align="center" style="font-size:small"><img src="/img/fc/fc9/fc9-14.jpg">훈련</p>
- 샘플 외 테스트의 경우, 이전 작업과의 일관성을 위해 P50 및 P90 위험에 초점을 맞추어 전체 예측 범위에 걸쳐 표준화된 분위수 손실을 평가하였다.
<p align="center" style="font-size:small"><img src="/img/fc/fc9/fc9-15.jpg">평가</p>

## 6. Performance evaluation
<img src="/img/fc/fc9/fc9-16.jpg">
- Dataset
    - Electricity : 과거 168시간으로 미래 24시간 예측, 370개 전력소의 전력사용량 데이터셋
    - Traffic : 440개의 freeway에 대한 사용률 데이터셋, 시간단위로 group핑하여 사용
    - Retail : 다른 상품들과 다른 store들의 메타데이터를 합친 Favorita 식료품점 판매량 데이터셋(캐글), 과거 90일을 사용해서 30일 예측
    - Volatility : 일일 수익과 함께 일별 데이터에서 계산된 31개 주가지수의 일일 실현 변동성 값이 수록된 데이터셋, 과거 1년치로 1주를 예측

- Training procedure
    - State size : 10, 20, 40, 80, 160, 240, 320
    - Dropout rate : 0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 0.9
    - Minibatch size : 64, 128, 256
    - Learning rate : 0.0001, 0.001, 0.01
    - Max, gradient norm - 0.01, 1.0, 100.0
    - num. heads - 1, 4

- Computational cost
    - single GPU에서도 학습이 될 만큼 resource를 많이 잡아먹지 않는다.
    - 예로, tesla V100이용시, Electricity 데이터셋은 학습은 6시간, 평가는 8분 정도 소요되었다.

- Bench mark
<img src="/img/fc/fc9/fc9-17.jpg">

## 7. Interpretability use cases
- 모델의 각 구성요소들의 설명력에 대해서 평가하고 있다.
<img src="/img/fc/fc9/fc9-18.jpg">

- Analyzing variable importance(Retail dataset)
    - 각 분위수에서 변수들의 영향력을 확인할 수 있다.(클수록 영향력이 높은 것이다.)
<img src="/img/fc/fc9/fc9-19.jpg">

- Visualizing persistent temporal patterns
    - Attention weights의 패턴은 TFT모델이 예측값을 결정하는 가장 중요하 과거 Step 요인을 알 수 있다.(사람의 개입없이 seasonality 파악 가능) → 모델의 신뢰성을 높여줄 수 있음
<img src="/img/fc/fc9/fc9-20.jpg">

- Identifying regimes & significant events
    - 급격한 시간 패턴의 변화는 중요한 사건의 존재 때문에 일어났음을 알 수 있기 때문에 유용할 수 있다.
    - 아래 그림을 보면, t2의 경우, 급격한 시계열의 변화가 발생했을 때, attention값이 증가함을 알 수 있다.(노란색)
<img src="/img/fc/fc9/fc9-21.jpg">

## 8. Conclusions
- TFT는 현실의 예측을 반영할 수 있는 충분한 조건을 갖췄다. 정적인 공변량과 과거 input 드리고 과거와 다른 갯수의 미래 input들을 중요한 변수선택과 다양한 mechanism을 통해 예측값을 얻을 수 있기 때문에 유용한 것이다.
- 또한 TFT는 설명력을 높여 [변수 간 영향력, 미래값에 영향을 주는 사람의 개입없는 시계열 패턴, 급격한 시계열의 변화](#7-Interpretability-use-cases)을 표현할 수 있기 때문에 user와 developer에게 유용할 것으로 예상한다.

