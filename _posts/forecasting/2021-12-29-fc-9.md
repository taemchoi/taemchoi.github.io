---
title: "TFT : Temporal FUsion Transformers for interpretable multi-horizon"
tags: [AI, Timeseries, 코드 구현, Forecasting]
categories:
  - Forecasting
date: 2021-12-29
toc : true
---

- [Abstract](#0-abstract)
- [Introduction](#1-introduction)
- [Related work](#2-related-work)
- [Multi-horizon forecasting](#3-multi-horizon-forecasting)
- [Model Architecture](#4-model-architecture)
  - [Gating mechanisms](#41-gating-mechanisms)
  - [Variable selection networks](#42-variable-selection-networks)
  - [Static covariate encoders](#43-static-covariate-encoders)
  - [Interpretable multi-head attention](#44-interpretable-multi-head-attention)
  - [Temporal fusion decoder](#45-temporal-fusion-decoder)
  - [Quantile outputs](#46-quantile-outputs)
- [Loss function](#5-loss-functions)
- [Performance evaluation](#6-performance-evaluation)
- [Interpretability use cases](#7-interpretability-use-cases)
- [Conclusions](#8-conclusions)

Multi-horizion forecasting이란, 단일 step을 예측하는 것이 아닌 여러 미래의 step을 예측하는 것

## 0. Abstract
- Multi-horizion forecasting은 복잡성이 혼합된 input ~(과거에에 관측된 타겟과 어떻게 연관되어있는지 사전 정보가 없는 다른 외인성(exogenous) 시계열과 미래의 input으로 알려진 정적 공변량)~ 을 종종 가지고 있다.
- 여러 딥러닝 method들이 제안되었지만 전형적으로 그것들은 실제 시나리오에서 현재 전체 길이의 인풋이 어떻게 사용되었는지 알 수 없는 'black-box' 모델이다.
- 그래서 해당 논문에서는 해석가능한 인사이트와 함게 고성능 multi-horizon forcasting을 할 수 있는 새로운 attention 기반의 mechanism을 갖고 있는 **TFT(Temporal Fusion Transformer)**를 소개한다.

## 1. Introduction
- Multi-horizon forecating은 시계열 기계학습에서 중요한 문제이다.
- 한 step만 예측하는 것과 달리, 사용자에게 전체 시계열에 대한 추정치를 접근가능하게 하고 이를 토대로, 그들이 원하는 미래의 여러 step에 대한 최적화된 행동을 할 수 있게 한다.
- 실제로 Multi-horizon forecasting은 retail, health-care, economics 등 실제 세계에 많은 영향을 끼치고 이런 method들의 성능향상은 매우 가치 있는 일이다.
- 실제 Multi-horizon forecasting은 일반적으로 서로 어떻게 상호작용하는지 모르는 다양한 데이터 원천(*미래에 대한 정보(휴일 등), 외인성 timeseries(유동인구 기록 등)정적 metadata(상점 위치 등)*)에 접근하게 된다.
- 상호작용에 대한 미약한 정보를 알고 있는 이런 데이터 원천들에 대한 이질성(heterogineity)은 Multi-horizon forecasting을 어렵게 한다.
- DNN은 이의 시계열 모델에 비해 강한 성능을 증명하고 있다. 그런데 많은 아키텍처들은 RNN 아케텍처에 초점을 맞추고 있고 최근 관련된 time step의 선택을 강화한 attention 기반의 method도 사용으로 개선되고 있다.
- 그런데 이것들은 외인성 input을 미래에 알 수 있다는 가정(자기회귀 모델의 일반적인 문제)이나 정적인 공변량을 무시한 가정(다른  time-dependent feature과 각 time step에서 결합) 등 흔히 있는 다른 type의 인풋들에 대한 도입 고려가 종종 실패하게 된다.
- 시계열 모델의 최근 발전은 데이터 고유한 특성을 갖고 있는 구조를 정렬함으로써 성과를 내었다.
- 우리는 적절한 귀납적 bias를 가진 모델을 설계함으로써 성과를 주장하고 증명하였다. 
- input의 이질성을 고려하지 않은 대부분의 현재 모델들은 파라미터간 비선형성 상호작용에 의해 예측하는 Blackbox 모델이다.
- 이는 모델이 어떻게 결과물을 가져왔는지 어렵게 하고 유저들이 결과를 신뢰하거나 개발자가 디버깅하는데도 어려움을 준다.
(설명가능한 DNN은 시계열에 적합하지 않다. - post-hoc 기법은 time step을 고려하지 않고 특징만 봄)
- **TFT**는 Attention 기반의 해석력을 제공하고 전체 구간의 잠재적인 인풋들의 시간관계를 가진 구조를 정렬하기 위해 아래의 4가지 idea를 제안하였다.
    1. **Static covariate encoder**(input context를 다른 network에서 사용하기 위해 encoding함)
    2. **Gating mechanism**(관계 없는 input들의 기여를 최소하는 전체와 표본의 종속 변수선택) 
    3. **Seq2seq layer**(알고 있는 관측된 input을 지역적으로 처리)
    4. **A temporal self attention decoder**(dataset의 long term dependency를 학습)

## 2. Related work
### 2.1. DNNs for Multi-horizon Forecasting
- 전통적 방법과 유사하게, 최근 딥러닝도 Autoregressive model를  사용한 iterated(반복되는) 접근과 seq2seq를 토대로한 directed method로 분류된다.
    1. **Iterated methods**
        - 한 스텝 예측을 반복해서 multi step의 예측값을 얻는다.(결과를 미래의 input으로 활용)
        - LSTM네트워크가 고려되는데 DEEPAR(가우시안 예측 분포의 파라마터를 생성하기 위해 LSTM LAYER를 쌓음), DSSM(Kalman filtering을 통해 생성된 예측 분포와 사전정의된 선형상태공간의 파라미터를 생성하기 위해 LSTM 활용) 등이 있다.
        - 최근에는 transformer 기반의 구조들이 예측시 local processing을 하기 위한 convolution layer와 예측 동안 receptive field를 늘리기 위한 sparse attenttion mechanism이 제시되었다.
        - 이런 단순함에도 불구하고 반복 기법은 타겟을 제외한 모든 변수는 미래값을 알 수 있다는 가정을 하고 있다.
        - 그러나, 많은 실제 시나리오 에서는 미래값을 알 수 없는 거대한 사용가능한 여러 시간이 존재한고 그렇기 때문에 실제 사용에는 제약이 따른다.
        - 반면, **TFT**는 입력의 다양성을 설명할 수 있다.
    2. **Direct methods**
        - 각 시간 단계에서 사전정의된 multiple horizon을 예측을 한번에 생성하도록 훈련된다.
        - 이런 구조는 전형적으로 seq2seq 모델에 의존한다.(과거를 요약하는 LSTM인코더와 미래를 생성하기 위한 다양한 methods)
        - MQRNN(Multi-horizon Quantile Recurrent forecaster)은 LSTM이나 conv 인코더를 이용하여 context vector를 만들고 이를 각각의 horizon에서 Multi layers perceptron에 feed하여 예측한다.
        - Multi-modal attention mechanism은 context vector를 만드는 LSTM encoder와 함께 Bi-directional LSTM의 디코더를 사용한다.
        - 1.의 반복 method에 비해 나은 성능에도 불구하고 여전히 설명력에 대한 문제가 남아있다.

### 2.2. Time Series Interpertability with Attention
- attention mechanism은 어텐션 가중치를 사용하여 각 instance에서 입력의 핵심적인 부분을 식별하기 위해 번역, 이미지 분류, tabular learning에서 많이 사용된다.
- 최근 LSTM과 Transformer의 구조를 기반으로한 attention mechanism은 시계열의 설명력을 위해 사용되었다.
- 그런데 static covarites를 고려하지 않고 사용하였다.
- TFT는 이러한 문제를 static feature들에 대하여 개별적인 encoder-decoder attention을 적용하여 해결하고자 한다.

### 2.3. Instance-wise Variable Importance with DNNs
- 변수의 중요도는 post-hoc method(LIME, SHAP, RL-LIM → 시계열을 설명하도록 설계되지 않음)와 inherently interpretable models(Interpretable Multi-variable LSTMs → global 시계열에 대한 설명이 아닌 sample에 대한 설명만 가능, 한 step에 대한 설명 가능)로 얻을 수 있다.
- TFT는 Global 시간 관계와 user가 dataset에 대한 전체 시간관계를 해석할 수 있게 해주었다.




