---
title: "GPT-3 - Language Models are Few-Shot Learners" 
tags: [NLP, AI]
categories:
  - nlp
  - sts
date: 2023-04-19
toc : true
---

[GPT3 논문](https://arxiv.org/pdf/2005.14165.pdf)

 ⎈ 참고사항

- AutoRegressive : 과거의 출력을 현재의 출력에 활용 ex) RNN
- Zero shot Learning : 소와 젖소 그리고 말의 이미지를 가르쳐주고 얼룩말은 젖소무늬를 가진 말이야 라고 힌트를 주면, 얼룩말을 한 번도 못 봤지만 얼룩말이라고 맞출 수 있게 됨
- One-shot Learning : 한 장의 사진으로 개념을 인식시켜 줌
- Few-shot Learning : 한 장이 아닌, 적은 양의 여러 장의 사진으로 개념을 인식시켜 줌

<img src="/img/nlp/nlp5/0.jpg">


1. Abstract
    - 기존의 NLP모델들은 자신의 Custom dataset을 가지고 Finetuning하는 구조였다.
    - 이 FineTuning 작업은 아래의 3가지 단점을 가지고 있다.
        - 첫째 : 실용적인 관점에서 보면, 새로운 task를 적용할 때 마다, 한 쌍의 데이터-라벨 세트를 가진 대규모 데이터 셋이 필요하기 때문에 언어 모델의 적용 가능성이 제한된다.
        - 둘째, 훈련 데이터의 허위 상관관계를 이용할 가능성은 모델의 표현력과 분포의 좁음에 따라 증가한다. 예를 들면, Pretrain에서 정보를 흡수할 수 있도록 모델을 크게 설계하고 매우 좁은 task의 분포에서 Fine-tuning한다면 Pretrain과 Fine-tuning간의  문제를 일으킬 수 있다. → 훈련 분포에 지나치게 특화되어있고 그 외의 분포에서는 일반화되지 않는 것을 의미
            - Spurious correlations(허위 상관) : 휴대폰 판매량이 증가하면 도시의 범죄건수가 증가한다. 그렇기 때문에 직접적 원인은 휴대폰 판매량이 증가이다.  사실은 중간에는 인구증가가 있다. 인구증가해서 판매량이 증가한 것이고 인구가 증가했기 때문에 범죄 건수가 증가한 것이지 둘 사이에 직접적인 상관성이 있는 것은 아니다.
        - 셋째, 인간의 학습 과정을 위배한다. 인간은 적은 양의 예시로도 합리적인 수준으로 답변을 할 수 있다. 장기적으로 봤을 때, 이런 관점을 따라가야 한다.
    - 즉, GPT3는 이런 틀에서 벗어나 Few-shot learning으로 해결한 것이 핵심 Contribution이다.
    - Meta Learning : 학습 중에 다양한 스킬과 패턴인식 능력을 향상시키는 것을 의미
        - 동일한 시퀀스에서는 유사 Context들이 들어가는 방식으로 학습시킴. 명시적으로 무슨 Task다 라고 알려주지 않아도 모델이 스스로 인식해서 다양한 스킬을 향상 시킨다.
        - GPT3에서의 In-context learning(Meta Learning) 예시
            
            <img src="/img/nlp/nlp5/1.jpg">
            
    - Naural Language Prompt vs No Prompt : Instruction을 주느냐의 차이
        - ex) add the two numbers, 1+2 vs 1+2
2. Approach
    1. 서론
        - FT(Fine-Tuning) : 많은 Sota를 달성했다는 장점이 있지만, 데이터 모으는 비용이 너무 드는 단점이 있음
        - FS(Few-shot) : 추론할 때, fewshot을 보여주는 것임 update는 없음, 장점은 새로운 테스트를 할 때, 데이터 수집 등에서 드는 비용을 감소시켰지만, 실제 성능은 FT보다 떨어진다는 단점이 있다.
        - 1S(one-shot)과 0S(zero-shot) : 1S는 실제 사진을 예시를 여러 번 보냐, 한 번 보냐의 차이이고 0S는 가장 허위 상관관계를 피할 수 있는 가능성을 지닌 방법이나, 이 논문에서는 성능을 보았을 때, FS를 선택하였다.
    2. Model and Architectures 
        - GPT-2와 동일한 모델구조를 가지고 있음(Initialization, Pre-normalization, reversible tokenization 포함) but, dense 와 transformer 레이어에서의 locally banded sparse attention를 번갈아 사용한다는 점에서 차이가 있음
            
            <img src="/img/nlp/nlp5/2.jpg">
            
            - GPT3 175B(GPT-3)는 96개의 layer, demension 12288, n head는 96, input token수 2048개
    
    <img src="/img/nlp/nlp5/3.jpg">
    
    1. Training Dataset 
        - Common Crawl dataset을 활용하였음 → 좋은 데이터만 뽑아쓰고 중복을 줄이고 좋은 데이터를 추가로 삽입해서 총 300B 개의 토큰을 모았음(중복 거의 없었다.)
        - 학습 디테일
        
        <img src="/img/nlp/nlp5/4.jpg">
        
3. Experiment
- 생성 부분에서는 좋은 성능을 가졌으나 기계독해 등 이해 부분에서는 bert에 비해 떨어짐
- 자세한 건 논문의 부분을 보시는게 좋을 듯    

4. Limitation
- 단방향 정보 취득이다 보니, 양방향 모델보다는 문장 이해력이 좀 떨어진다.