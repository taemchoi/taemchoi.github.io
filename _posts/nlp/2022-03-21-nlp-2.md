---
title: "STS 프로젝트 1탄 : 'Sentence-BERT : Sentence Embeddings using Siamese BERT-Networks'"
tags: [NLP, AI]
categories:
  - nlp
  - sts
date: 2022-03-21
toc : true
---

- [Abstract & Introduction & Related work](#1-abstract--introduction--related-work)
  - [Abstract](#11-abstract)
  - [Introduction](#12-introduction)
  - [Related Work](#13-related-work)
- [Models](#2-models)
  - [Classification Objective Function](#21-classification-objective-function)
  - [Regression Objective Function](#22-regression-objective-function)
  - [Triplet Objective Function](#23-triplet-objective-function)
- [Evaluation - Semantic Textual Similarity](#3-evaluation---semantic-textual-similarity)
  - [UnSupervised STS](#31-unsupervised-sts)
  - [Supervised STS](#32-supervised-sts)
- [Evaluation - SentEval](#4-evaluation---senteval)
- [Ablation Study](#5-ablation-study)
- [Computational Efficiency](#6-computational-efficiency)

## 1. Abstract & Introduction & Related work
### 1.1. Abstract
- 기존의 Bi-encoders(Bert base)의 추론 연산은 문장이 10000개라고 한다면 <sub>10000</sub>C<sub>2</sub>로 약 50 million 계산이 필요하다.(~ 65시간)
- `SBERT`는 이를 효과적으로 줄여 성능은 유지하면서 **5sec**로 시간을 단축시켰다.

### 1.2. Introduction
- 뒤에서 모델 구조는 자세히 다루겠지만, **siamse and triplet netowrks 구조**를 활용하여 임베딩을 이끌어 냈다.  
→ BERT가 하지 못하는 large scale의 smilarity comparison, clustering, information retrieval 등을 할 수 있게 되었다.
- 기존 BERT는 CROSS-ENCODERS로 시간효율성 측면에서는 최악이다.  
→ context와 candidate를 계속 이어 붙여서 활용(동일한 context도 계속 임베딩 cost 발생)
- 고정된 size로 embedding을 할 때, 2가지 접근법이 주로 활용된다.
    - 마지막 layer를 평균한다.(avg pooling)
    - output의 첫번째 토큰([CLS])을 활용한다.
- 하지만, 때때로, *GLoVe 임베딩의 평균보다 못한 결과를 가져온다.*
- 개선점
    - siamese network : 입력문장에 대한 고정된 크기의 vector를 유도할 수 있게 되었다.
    - 유사도 측정 방법을 모델에 도입함으로써 유사문장을 찾을 수 있었다.  
    → 이 측정 방법은 현대 하드웨어에서 효과적으로 작동하며 semantic similarity search나 clustering에도 활용할 수 있게 되었다.

- 제시한 모델 SBERT는 NLI 데이터셋으로 fine-tuning 했다.
- InferSent나 Universal Sentence Encoder보다 좋은 성능으로 SOTA를 차지하였다.

### 1.3 Related Work
1. BERT
    - senetnence pair regression에서 두 문장은 [SEP]로 구분하여 구성된다.
    - Multi-head attention(12 or 24)를 적용하고 output layer로 simple regression layer를 지나 최종 label을 출력한다.
    - 이런 setup으로 STS TASK SOTA를 달성하였고 RoBERTa의 경우, 작은 pre-training process를 적용하여 BERT를 향상하였다.
    - XLNet은 BERT보다 좋지 않은 결과를 냈다.
    - BERT 큰 단점은 자체적으로 문장 임베딩이 계산될 수 없어 단일 문장의 임베딩을 계산할 수 없다는 것이다.
    - 이 한계를 우회하기 위해, output의 평균(average word embedding과 유사)을 하거나 CLS token의 출력을 이용하는 방법을 많이 활용하였다.  
    → 이러한 방법이 sentence embedding에 유용한지는 제대로 평가 되지 않았다.(실제 GLoVe 보다 좋지 않은 경우 많음)

2. Sentence Embeddings
    - **Skip-Throught**는 주변 문장을 예상하기 위해 encoder-decoder 구조로 train 되었다.
    - **InferSent**는  *Stanford NLI, Multi-Genre NLI dataset*을 출력에 average max-pooling을 적용한 siamese BiLSTM network로 train 한 모델이다. (InferSent는 Skip-Throught 같은 비지도 학습 모델보다 뛰어나다는 논문도 있다.)
    - **Universal Sentence Encoder**는 transformer network를 훈련시키고 SNLI dataset을 비지도 학습으로 훈련시켜 강화하였다.
    - pre-train task가 중요하다는 연구가 있고 또, SNLI가 STS TASK에 적합하다는 연구도 있다.
    - siamese DAN과 siamese transformer network를 이용해 Reddit의 대화를 학습한 연구도 있으며, 이는 STS 벤치마크에서 좋은 결과를 가져왔다.
    - cross-encoder와 Bi-encoder의 장점을 취한 **poly-encoder**모델도 있다.(<a href="https://taemchoi.github.io/nlp/nlp-3/">poly-encoder post 참조</a>)  
    → 큰 데이터셋에서 Top1을 찾는데는 효율적이지만, Score function이 비대칭적이고 clustering 같은 task에서는 overhead가 너무 많이 걸린다.(O(n<sup>2</sup>))
    - 이전의 딥러닝 sentence embedding method들은 random initialization을 사용하였으나, **SBERT**는 기존의 pre-trained BERT(RoBERTa)를 fine-tune만 적용하여 좋은 결과를 얻었다.  
    → **학습하는데 20분도 안걸리나 성능은 좋음**

## 2. Models
- *pooling layer* : output에 고정된 크기의 sentence embedding을 가져오기 위해 pooling layer를 추가하였다.(CLS 토큰을 결과에 사용, avg-pooling, max-over-time을 계산, default는 avg-pooling)
- *siamese and triplet network* : sentence embedding의 가중치를 update하고 cos-similarity를 계산될 수 있도록 하였다.  
    - siamese(샴 네트워크) : 모델의 가중치를 공유하는 network(즉, 하나의 모델로 두 개의 input으로 부터 두 개의 임베딩 값을 계산하고 이 두 결과를 aggregate(cos-sim, softmax(concat) 등)하여 네트워크를 학습 시킴  
    <p align='center' style="font-size:small"><img src="/img/nlp/nlp2/3.jpg"><a href="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf">siamese논문</a></p>  

    - triplet(트리플렛 네트워크) : 입력값이 3개로 하나는 기준이 되는 anchor, anchor와 동일한 label을 가진 positive, anchor와 다른 label을 가진 negative로 구성, anchor와 positive의 거리는 가깝게 negative는 멀어지도록 학습시킴  
    <p align='center' style="font-size:small"><img src="/img/nlp/nlp2/4.jpg"><a href="https://arxiv.org/pdf/1412.6622.pdf">triplet논문</a></p>

- 모델 구조는 task에 따라 구조와 목적함수를 다르게 하였다.

### 2.1. Classification Objective Function
<img src="/img/nlp/nlp2/0.jpg">

- u, v, ｜u-v｜<sub>(element-wise difference)</sub>를 concatenate한 결과를 W<sub>t</sub> ∈ R<sup>3n x κ</sup>와 곱하고 softmax를 취한 것이 목적함수 o 이다.(n = sentence-emb dim, k = label)
- cross enthropy로 학습한다.
- 수식 : **o = softmax(W<sub>t</sub>(u, v, ｜u-v｜))**

### 2.2. Regression Objective Function
<img src="/img/nlp/nlp2/2.jpg">

- u,v의 cos-similarity를 계산하였고 MSE-loss를 사용하였다.

### 2.3. Triplet Objective Function
- anchor sentence **a**, positive sentence **p**, negative sentence **n**이 주어지면, triplet loss는 (a,p) 거리를 (a,n)보다 작아지도록 학습 작용한다.
- 수식 : **max(∣∣s<sub>a</sub>​−s<sub>p</sub>​∣∣−∣∣s<sub>a</sub>​−s<sub>n</sub>∣∣+ϵ, 0)**
- s는 문장 임베딩이고, ∣∣⋅∣∣는 거리고, ϵ은 margin이다. margin은 s<sub>p</sub>가 s<sub>n</sub>보다 최소 ϵ만큼 s<sub>a</sub>에 가깝도록 하기 위해 도입하였다.(default ε=1)

## 3. Evaluation - Semantic Textual Similarity
- 기존 모델들은 유사도를 산출할 때, regression function을 이용했는데, 이는 pair-wise하기에 문장 집합이 커지면 조합이 폭발적으로 증가했다.(<sub>n</sub>C<sub>2</sub>)
- SBERT는 Cos-similarity를 사용하는데 negative manhatten이나 negative Euclidean도 실험하였으나 비슷한 결과를 가져왔다.

### 3.1. UnSupervised STS
<img src="/img/nlp/nlp2/5.jpg">

### 3.2. Supervised STS
<img src="/img/nlp/nlp2/6.jpg">

- NLI base, STSb base, NLI(Pre-train)-STSb(Fine-tune) 세 가지 학습시나리오에 대한 결과 평가

## 4. Evaluation - SentEval
<img src="/img/nlp/nlp2/7.jpg">

<img src="/img/nlp/nlp2/8.jpg">

## 5. Ablation Study
<img src="/img/nlp/nlp2/9.jpg">

- Ablation Study는 여라 경우의 수를 비교하며 성능을 비교하는 것을 의미한다.

- 위에서는 pooling의 3가지 전략, concatenation option을 바꿔가며 성능지표를 측정하였다.

- 중요한 것은 ｜u-v｜이다. element-wise difference는 두 임베딩 사이의 거리를 측정하는데, 이는 비슷한 pair는 더 가깝게, 안 비슷한 pair는 더 멀게하는 효과가 있다.

## 6. Computational Efficiency
<img src="/img/nlp/nlp2/10.jpg">