---
title: "InstructGPT - Training Language models to follow instructions with human feedback" 
tags: [NLP, AI]
categories:
  - nlp
  - llm
date: 2023-04-21
toc : true
---

[InstructGPT 원본](https://arxiv.org/pdf/2203.02155.pdf)

<aside>
💡 **GPT-1**(2018.06.11) → **GPT-2**(2019.02.14) → **GPT-3**(2020.05.28) → **InstructGPT**(2022.05.24) → **ChatGPT**(GPT-3.5)(2022.11.30) → **GPT-4**(2023.03.15)

</aside>

### 1. Abstract

- 언어모델을 더 크게 만든다고 해서 본질적으로 사용자의 의도를 더 잘 따르는 것은 아니다.
- GPT3와 같이 대규모 언어모델은 여전히 신뢰할수 없거나 도움이 안되는 결과를 User에게 주고 있다.
- 이는 **Aligned** 되지 않아서 그렇고 우리는 Human Feedback으로 강화학습을 통해 fine-tune된 `InstructGPT` 를 제시한다. → **Alignment 필요(인간의 의도에 맞게 출력하도록 조정)**
- 실제 실험에서 GPT3 - 175B 보다 InstructGPT 1.3B 가 더 나은 결과를 보여주었다.

### 2. Introduction

<img src="/img/nlp/nlp6/0.jpg">

- 의도에 따라 언어모델이 명확하게 작동하게 해야함
    - 명시적 의도 : 사용자의 의도를 따르도록 하는 것
    - 암시적 의도 : 신뢰감있고 유해한 정보를 제공하지 않는 것
- Labeler를 40명을 고용하여 모델 성능향상에 참여시킴

### 3. Method

- step 1. **Supervised fine-tuning**
    - 학습 데이터 수집 : OpenAI API의 Playground I/F에서 수집
        
        <img src="/img/nlp/nlp6/1.jpg">
        
        <img src="/img/nlp/nlp6/2.jpg">
        
    - 라벨러가 바람직한 demostration을 작성
    - GPT - 3 모델 지도학습 → `SFT Model`
- step 2. **Reward Modeling**

<img src="/img/nlp/nlp6/3.jpg">

<img src="/img/nlp/nlp6/4.jpg">

- Prompt**(Context)**를 SFT Model**(Policy)**를 이용하여 답변**(continuation)** 4개~9개 사이 생성
- 라벨러의 답변 선호도 랭킹화(Label)
- 마지막에 unembedding layer를 제거한 SFT Model을 학습(input : prompt와 response / output : reward score) → `Reward Model`
- Reward Model은 6B GPT-3 활용 → 175B GPT-3 는 강화학습에서 value function으로 사용하기 적합하지 않음 그리고 Unstable함
- 두 개의 답변을 비교하는 방법으로 학습 진행 (kC2 개 조합 발생)
    - 랭킹을 비교하는 방법이 bias, varience를 줄일 수 있다고 함

<img src="/img/nlp/nlp6/5.jpg">

- step 3. **Reinforement Learning PPO-ptx : [step 1]의 SFT모델을 [step 2]의 RM을 활용하여 ppo-ptx를 사용하여 파라미터 튜닝 진행**
    - 마지막이 term(D-pretrain 있는 것)이 ptx → GPT3의 데이터를 가져와서 강화학습 중에도 계속 같이 생성 해줌
        
        → public NLP 데이터셋에서 성능 저하 문제가 발생했는데 이를 최소하기 위한 정책을 추가함
        

<img src="/img/nlp/nlp6/6.jpg">

<img src="/img/nlp/nlp6/7.jpg">

### 4. Evaluation

- Helpfulness, Truthfulness(hallucinations), Hamful 3가지 부분을 주요하게 평가함

<img src="/img/nlp/nlp6/8.jpg">

![Untitled](InstructGPT%20-%20%E2%80%9CTraining%20Language%20models%20to%20follow%20%20b1b62cfeb72f4693a2c89bc45b3329b5/Untitled%208.png)

### 5. lessons for alignment research

1.  The cost of increasing model alignment is modest relative to pretraining.
    - training our 175B SFT model requires 4.9 petaflops/s-days and training our 175B PPO-ptx model requires 60 petaflops/s-days, compared to 3,640 petaflops/s-days for GPT-3
2. We’ve seen some evidence that InstructGPT generalizes ‘following instructions’ to settings that we don’t supervise it in.
    - for example on non-English language tasks and code-related tasks
3. We were able to mitigate most of the performance degradations introduced by our fine-tuning

### 6. Limitation

1. value judgement 할 때, 레이블러의 신념 등이 관여하기에 취약점이 있을 것이다.