---
title: "InstructGPT - Training Language models to follow instructions with human feedback" 
tags: [NLP, AI]
categories:
  - nlp
  - llm
date: 2023-04-21
toc : true
---

[InstructGPT ì›ë³¸](https://arxiv.org/pdf/2203.02155.pdf)

<aside>
ğŸ’¡ **GPT-1**(2018.06.11) â†’ **GPT-2**(2019.02.14) â†’ **GPT-3**(2020.05.28) â†’ **InstructGPT**(2022.05.24) â†’ **ChatGPT**(GPT-3.5)(2022.11.30) â†’ **GPT-4**(2023.03.15)

</aside>

### 1. Abstract

- ì–¸ì–´ëª¨ë¸ì„ ë” í¬ê²Œ ë§Œë“ ë‹¤ê³  í•´ì„œ ë³¸ì§ˆì ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ë” ì˜ ë”°ë¥´ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤.
- GPT3ì™€ ê°™ì´ ëŒ€ê·œëª¨ ì–¸ì–´ëª¨ë¸ì€ ì—¬ì „íˆ ì‹ ë¢°í• ìˆ˜ ì—†ê±°ë‚˜ ë„ì›€ì´ ì•ˆë˜ëŠ” ê²°ê³¼ë¥¼ Userì—ê²Œ ì£¼ê³  ìˆë‹¤.
- ì´ëŠ” **Aligned** ë˜ì§€ ì•Šì•„ì„œ ê·¸ë ‡ê³  ìš°ë¦¬ëŠ” Human Feedbackìœ¼ë¡œ ê°•í™”í•™ìŠµì„ í†µí•´ fine-tuneëœ `InstructGPT` ë¥¼ ì œì‹œí•œë‹¤. â†’ **Alignment í•„ìš”(ì¸ê°„ì˜ ì˜ë„ì— ë§ê²Œ ì¶œë ¥í•˜ë„ë¡ ì¡°ì •)**
- ì‹¤ì œ ì‹¤í—˜ì—ì„œ GPT3 - 175B ë³´ë‹¤ InstructGPT 1.3B ê°€ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆë‹¤.

### 2. Introduction

<img src="/img/nlp/nlp6/0.jpg">

- ì˜ë„ì— ë”°ë¼ ì–¸ì–´ëª¨ë¸ì´ ëª…í™•í•˜ê²Œ ì‘ë™í•˜ê²Œ í•´ì•¼í•¨
    - ëª…ì‹œì  ì˜ë„ : ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ë”°ë¥´ë„ë¡ í•˜ëŠ” ê²ƒ
    - ì•”ì‹œì  ì˜ë„ : ì‹ ë¢°ê°ìˆê³  ìœ í•´í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì§€ ì•ŠëŠ” ê²ƒ
- Labelerë¥¼ 40ëª…ì„ ê³ ìš©í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥í–¥ìƒì— ì°¸ì—¬ì‹œí‚´

### 3. Method

- step 1. **Supervised fine-tuning**
    - í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ : OpenAI APIì˜ Playground I/Fì—ì„œ ìˆ˜ì§‘
        
        <img src="/img/nlp/nlp6/1.jpg">
        
        <img src="/img/nlp/nlp6/2.jpg">
        
    - ë¼ë²¨ëŸ¬ê°€ ë°”ëŒì§í•œ demostrationì„ ì‘ì„±
    - GPT - 3 ëª¨ë¸ ì§€ë„í•™ìŠµ â†’ `SFT Model`
- step 2. **Reward Modeling**

<img src="/img/nlp/nlp6/3.jpg">

<img src="/img/nlp/nlp6/4.jpg">

- Prompt**(Context)**ë¥¼ SFT Model**(Policy)**ë¥¼ ì´ìš©í•˜ì—¬ ë‹µë³€**(continuation)** 4ê°œ~9ê°œ ì‚¬ì´ ìƒì„±
- ë¼ë²¨ëŸ¬ì˜ ë‹µë³€ ì„ í˜¸ë„ ë­í‚¹í™”(Label)
- ë§ˆì§€ë§‰ì— unembedding layerë¥¼ ì œê±°í•œ SFT Modelì„ í•™ìŠµ(input : promptì™€ response / output : reward score) â†’ `Reward Model`
- Reward Modelì€ 6B GPT-3 í™œìš© â†’ 175B GPT-3 ëŠ” ê°•í™”í•™ìŠµì—ì„œ value functionìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ì í•©í•˜ì§€ ì•ŠìŒ ê·¸ë¦¬ê³  Unstableí•¨
- ë‘ ê°œì˜ ë‹µë³€ì„ ë¹„êµí•˜ëŠ” ë°©ë²•ìœ¼ë¡œ í•™ìŠµ ì§„í–‰ (kC2 ê°œ ì¡°í•© ë°œìƒ)
    - ë­í‚¹ì„ ë¹„êµí•˜ëŠ” ë°©ë²•ì´ bias, varienceë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤ê³  í•¨

<img src="/img/nlp/nlp6/5.jpg">

- step 3. **Reinforement Learning PPO-ptx : [step 1]ì˜ SFTëª¨ë¸ì„ [step 2]ì˜ RMì„ í™œìš©í•˜ì—¬ ppo-ptxë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒë¼ë¯¸í„° íŠœë‹ ì§„í–‰**
    - ë§ˆì§€ë§‰ì´ term(D-pretrain ìˆëŠ” ê²ƒ)ì´ ptx â†’ GPT3ì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ê°•í™”í•™ìŠµ ì¤‘ì—ë„ ê³„ì† ê°™ì´ ìƒì„± í•´ì¤Œ
        
        â†’ public NLP ë°ì´í„°ì…‹ì—ì„œ ì„±ëŠ¥ ì €í•˜ ë¬¸ì œê°€ ë°œìƒí–ˆëŠ”ë° ì´ë¥¼ ìµœì†Œí•˜ê¸° ìœ„í•œ ì •ì±…ì„ ì¶”ê°€í•¨
        

<img src="/img/nlp/nlp6/6.jpg">

<img src="/img/nlp/nlp6/7.jpg">

### 4. Evaluation

- Helpfulness, Truthfulness(hallucinations), Hamful 3ê°€ì§€ ë¶€ë¶„ì„ ì£¼ìš”í•˜ê²Œ í‰ê°€í•¨

<img src="/img/nlp/nlp6/8.jpg">

![Untitled](InstructGPT%20-%20%E2%80%9CTraining%20Language%20models%20to%20follow%20%20b1b62cfeb72f4693a2c89bc45b3329b5/Untitled%208.png)

### 5. lessons for alignment research

1.  The cost of increasing model alignment is modest relative to pretraining.
    - training our 175B SFT model requires 4.9 petaflops/s-days and training our 175B PPO-ptx model requires 60 petaflops/s-days, compared to 3,640 petaflops/s-days for GPT-3
2. Weâ€™ve seen some evidence that InstructGPT generalizes â€˜following instructionsâ€™ to settings that we donâ€™t supervise it in.
    - for example on non-English language tasks and code-related tasks
3. We were able to mitigate most of the performance degradations introduced by our fine-tuning

### 6. Limitation

1. value judgement í•  ë•Œ, ë ˆì´ë¸”ëŸ¬ì˜ ì‹ ë… ë“±ì´ ê´€ì—¬í•˜ê¸°ì— ì·¨ì•½ì ì´ ìˆì„ ê²ƒì´ë‹¤.