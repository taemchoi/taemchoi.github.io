---
title: "Transformer : Attention Is All You Need"
tags: [NLP, AI]
categories:
  - nlp
date: 2021-12-24
toc : true
---

- [Introduction](#1-introduction)
- [Background](#2-background)
- [Model Architecture](#3-model-architecture)
  - [Encoder and Decoder Stacks](#31-encoder-and-decoder-stacks)
  - [Attention](#32-attention)
  - [Postion-wise Feed-Forward Networks](#33-postion-wise-feed-forward-networks)
  - [Embedding and Softmax](#34-embedding-and-softmax)
  - [Positional Encoding](#35-positional-encoding)
- [Why Self-Attention](#4-why-self-attention)
- [Explaination of Model Dimensions](#5-explaination-of-model-dimensions)
- [Example with visuallization](#6-example-with-visuallization)


## 1. Introduction
- RNN, LSTM, GRU 등은 sequence 모델링과 변환 문제(Language modeling 과 기계번역)에서 sota를 달성했따.
- 반복모델은 전형적으로 input과 output sequence의 symbol position에 따라 계산을 수행한다. 
    - 계산 시간의 단계에 위치들을 정렬하고 rnn계열의 모델들은 이전 hidden state(h<sub>t-1</sub>)와 t 시간의 입력으로 hidden state(h<sub>t</sub>)를 생성한다.
    - 이는 본질적으로 메모리제약으로 인해 긴 sequence가 중요한 훈련 예제에서 **병렬 처리를 못하게 된다.**
    - 최근 factorization tricks과 조건부 계산을 활용한 계산적 효과로 충분한 향상을 이뤘지만 여전히 근본적인 위와같은 문제는 남아있다.
- **Attention mechanism**은 입력과 출력 sequence의 거리에 관계없이 의존성을 모델링함으로써 다양한 과제에서의 sequence 모델링과 변환 모델에서 중요한 부분이 되었다. 그런데 몇가지 경우를 제외하고 Recurrent 모델과 함께 사용되고 있다.
- <span style="color:blue">**Transformer**</span>는 Recurrence를 회피하고 input과 output 사이의 global 의존성을 학습할 수 있는 attention mechanism 만을 사용하는 모델이다.
    - 충분한 병렬처리를 가능하게 한다.
    - P100 GPU 8개만으로 12시간이하로 학습하여 번역 부분에서 sota를 달성하였다.

## 2. Background
- 연속적인 계산을 줄이는 목표로 모델링된 Extended Neural GPU, ByteNet, ConvS2S들은 모두 컨볼루션 네트워크 골격으로 사용한다.
    - 이러한 모델들은 input과 output의 위치로 부터 관계성을 얻기 위한 작동은 convS3S는 선형적으로 ByteNet은 로그비례하게 input output 사이의 거리에 따라 증가하게 된다. 이는 장거리 의존성을 학습하는데 더어렵게 된다. 
    - <span style="color:blue">**Transformer**</span>에서는 계산을 줄일 수 있었다.(비록, averaging attention-weighted postions 때문이긴 하나, **Multi-Head-Attention** 효과이기도 하다.)
- intra-attention이라고 불리기도하는 **Self-Attention**은 seqence의 representation을 계산하기 위한 단일 sequence의 다른 위치를 연관시키는 attention mechanism이다.
- End-to-end 메모리 네트워크는 sequence-aligned recurrence 대신 recurrent attention mechanism에 기반하였으며 simple-language QA와 Language modeling task 등에서 성과를 보였다. 
- 그러나, **Transformer는 sequence를 정렬하는 RNN 혹은 CNN 없이 self-attention만으로 input과 output의 representation을 계산한 첫번째 변환 모델이다.**

## 3. Model Architecture
- 경쟁력있는 뉴럴 sequence 변환 모델들은 encoder-decoder 구조를 갖고 있다. 
    - 인코더의 inpurt인 symbol representations **x**(x<sub>1</sub>, ..., x<sub>n</sub>)를 continuous representations **z**(z<sub>1</sub>, ... , z<sub>n</sub>) 에 맵핑한다. 디코더는 **z**가 주어지면 한번에 한 원소씩 output sequence y(y<sub>1</sub>, ... , y<sub>m</sub>)을 생성한다.
    - 각 단계는 auto-regressive이며, 이전에 생성된 symbol(y<sub>m-1</sub>)를 다음 단계(y<sub>m</sub>)를 생성할 때 추가 입력으로 한다.

  - **Transformer**는 인코더와 디코더 모두에 fully connected layers, self-attention 과  point wise를 사용하며 그림은 아래와 같다.
  
<img src="/img/nlp1-0.jpg">

```python
class Transformer(nn.Module):
    def __init__(self, input_vocab_size, num_encoder_layers, d_model, n_head, drop_out, num_decoder_layers, target_vocab_size, max_length):
        super(Transformer, self).__init__()
        self.enc = Encoder(num_encoder_layers, d_model, n_head, drop_out)
        self.dec = Decoder(d_model, n_head, drop_out, num_decoder_layers) 
        # self.embed = preprocessing_fc_layer(input_feature=input_feature, d_model=d_model, num_category=num_category, embedding_dim=embedding_dim)
        self.embed = nn.Embedding(input_vocab_size, d_model)
        self.pe = PositionalEncoding(d_model, max_length)
        self.fc_1 = nn.Linear(d_model, target_vocab_size)
        self.softmax = nn.SoftMax(target_vocab_size)
        
    def __call__(self, enc_input, dec_input, category):
        enc_mask = get_attention_pad_mask(enc_input, enc_input)
        dec_mask = torch.gt((get_attention_pad_mask(dec_input, dec_input) + get_attn_subsequent_mask(dec_input), 0))
        enc_dec_mask = get_attention_pad_mask(dec_input, enc_input) 
        embed_enc_input = self.embed(enc_input, category)
        enc_output = self.enc(embed_enc_input, enc_mask)
        dec_input = self.embed(dec_input, category)
        dec_output = self.dec(enc_output, dec_input, enc_dec_mask, dec_mask)
        output = self.Softmax(self.fc_1(dec_output))
        return output 
```

### 3.1. Encoder and Decoder Stacks
- **Encoder** 
    - 인코더는 동일한 6개의 layer가 쌓여있다.  
    - 각 layer는 2개의 sub-layer로 구성되어 있다.
        1. <u><b>Multi-head self attention mechanism</b></u>
        2. <u><b>Position-wise fully connected feed-forward netowrk</b></u>
    - 2개의 sub-layer들은 sub layer 끝의 <u><b>Layer normalization</b></u>에서 각각 <u><b>Residual connection</b></u>을 활용한다.
        - sub-layer의 출력 값은 *LayerNorm(x + Sublayer(x))* 가 된다.(Sublayer(x)는 sublayer 자체로 구현되는 함수이다.)
        - Residual connection을 용이하게 하기 위해, <u><b>embedding layer</b></u>를 포함한 모든 sub-layer들은 output을 <span style="color:green">**512차원**</span>으로 한다.

```python
class Encoder(nn.Module):
    def __init__(self,num_encoder_layers, d_model, n_head, drop_out):
        super(Encoder,self).__init__()
        self.encoder_layers = nn.Sequential()
        for i in range(num_encoder_layers):
            self.encoder_layers.add_module(f'encoder{i}', Encoder_layer(d_model, n_head, drop_out)) 
    
    def __call__(self, x, enc_mask):
        out = self.encoder_layers(x, enc_mask)
        return out

class Encoder_layer(nn.Module):
    def __init__(self, d_model, n_head, drop_out):
        super(Encoder_layer, self).__init__()
        self.enc_attention=Multi_head_attention(d_model, n_head)
        self.pff_network = Position_wise_feed_forward(d_model, drop_out)
        
    def __call__(self, x, enc_mask):
        """
        Args:
            x ([Tensor]): positional embedding까지 완료된 tensor(batch, sequence, d_model)

        Returns:
            out ([Tensor]): encoder의 output tensor(batch, sequence, d_model) 
        """
        out = self.enc_attention(q=x, k=x, v=x, mask=enc_mask)
        out = self.pff_network(out)
        return out, enc_mask
```


- **Decoder**
    - 디코더는 동일한 6개의 layer가 쌓여있다. 
    - 2개의 sub-layer 외에 1개를 추가로 가져 총 3개의 sub-layer를 가진다.
        1. <u><b>Multi-head self attention mechanism</b></u>
        2. <u><b>Position-wise fully connected feed-forward netowrk</b></u>
        3. <u><b><BIG>Masked Multi-head self attention mechanism</BIG></b></u>
    - 인코더와 마찬가지로 <u><b>Residual connection</b></u>을 <u><b>Layer normalization</b></u>에서 사용하고 있다.
    - <u><b>Masked Multi-head self attention mechanism</b></u>은 decoder에서 다음 위치의 정보를 활용하는 것을 방지하기 위해 masking을 하였다.
    - 이 masking은 postion<sub>i</sub>를 예측할 때, position<sub>1</sub> ,..., position<sub>i-1</sub>만 활용하게 된다.

```python
class Decoder(nn.Module):
    def __init__(self,  d_model, n_head, drop_out, num_decoder_layers):
        super(Decoder, self).__init__()
        self.decoder_layers = nn.Sequential()
        for i in range(num_decoder_layers):
            self.decoder_layers.add_module(f'encoder{i}', Decoder_layer(d_model, n_head, drop_out)) 
        
    def __call__(self, enc_ouput, dec_input, enc_dec_mask, dec_mask):
        output = self.decoder_layers(enc_ouput, dec_input, dec_mask, enc_dec_mask)
        return output

class Decoder_layer(nn.Module):
    def __init__(self, d_model, n_head, drop_out):
        super(Decoder_layer, self).__init__()
        self.masked_dec_attention=Multi_head_attention(d_model, n_head)
        self.enc_dec_attention=Multi_head_attention(d_model, n_head)
        self.pff_network = Position_wise_feed_forward(d_model, drop_out)
        
    def __call__(self, encoder_output, decoder_input, dec_mask, enc_dec_mask):
        output = self.masked_dec_attention(q=decoder_input,k=decoder_input,v=decoder_input, mask=dec_mask)
        output = self.enc_dec_attention(q=output, k=encoder_output, v=encoder_output, mask=enc_dec_mask)
        output = self.pff_network(output)
        return encoder_output, output, dec_mask, enc_dec_mask
```

### 3.2. Attention
- attention function은 Query(Q)와 한 쌍의 Key(K), value(V)를 output에 맵핑한다.(Q,K,V,output 모두 벡터이다.)
- output은 Q와 일치하는 K의 Compatibility function(호환성 함수)에 의해 계산되서 각각의 V 할당된 가중치들의 가중합으로 구하게 된다. 
<img src="/img/nlp1-2.jpg">

#### 3.2.1 Scaled Dot-Product Attention
위 그림에서 왼쪽에 해당한다.
- Input은 **Query, Key(dimension=d<sub>k</sub>), Value(dimension=d<sub>v</sub>)**로 구성되어 있다.
- 계산과정
    1. Query를 모든 Key와 Dot production한다.
    2. 각각 d<sub>k</sub><sup>1/2</sup>로 나눈다.
    3. Softmax function을 적용하여 가중치(w)를 구한다.
    4. w와 V를 곱해서 Attention score를 구한다.
- Query들을 동시에 계산하기 위해 행렬 Q로 pack 하고 Key와 Value도 행렬 K, 행렬 V로 묶게된다.
- 수식을 보면 아래와 같다.
<img src="/img/nlp1-3.jpg">

- 많이 쓰이는 attention은 2가지가 있다.
    1. additive attention
        - single hidden layer와 feed-forward network를 활용해서 계산한다.
        - d<sub>k</sub>가 작을 때 성능이 좋다.
    2. dot-product attention
        - 최적화된 행렬곱 알고리즘을 사용해서 additive attention 보다 빠르고 더 공간 효율적이다.
        - d<sub>k</sub>가 클 때 성능이 좋다.
- d<sub>k</sub>가 큰 경우, dot product는 더 큰 규모로 커지게 되고, 이는 극단적으로 작은 기울기를 가지는 영역을 softmax function에 넣게 된다. <br>이런부분에서 발생하는 효과에 대응하기 위해 스케일링의 일환으로 d<sub>k</sub>로 나눈다.<br><sub>(q와 k의 요소들을 평균이 0이고 분산이 1인 분포라고 가정하면, dot product(q,k)의 평균은 0 분산은 d<sub>k</sub>를 따르게 됨, 이를 나눠주는 것은 정규화 시켜주는 효과로 볼 수 있음)</sub>


```python
class Scaled_dot_product_attention(nn.Module):
    def __init__(self):
        super(Scaled_dot_product_attention, self).__init__()
        self.softmax = nn.Softmax(dim=1)
    
    def __call__(self, q, k, v, mask=None):
        d_k = k.size(-1)
        attention_key = torch.matmul(q,k.transpose(-2,-1))/np.sqrt(d_k) # batch_size, n_head, sequence, sequence
        if mask:
            attention_key=attention_key.masked_fill_(mask, 1e-12) # batch_size, n_head, sequence, sequence
        attention_score = torch.matmul(self.softmax(attention_key), v)  # batch_size, n_head, sequence, d_model/n_head
        return attention_score
```

#### 3.2.2 Multi-Head Attention
오른쪽 그림에 해당한다.
- d<sub>model</sub> 차원 key, value, query로 단일 attention fuction으로 수행하는 것보다, Query, Key, Value를 h번 다른 d<sub>k</sub>, d<sub>k</sub>, d<sub>v</sub>차원으로 각각 학습시키는 것이 유리하다.
    - 모든 학습은 병렬로 진행하고 d<sub>v</sub> 차원을 출력한다.
- 이 결과들을 concat하고 한번더 project함으로써 최종 결과물을 출력하게 된다.
<img src="/img/nlp1-4.jpg">
- 계산과정
    1. Q,K,V를 Linear 연산하여 Q, K, V 행렬로 변환
    2. 병렬로 scaled Dot-product-Attention하여 각각의 Representation space를 생성
    3. Concatenate하여 연결
    4. linear 다시 project 함으로써 512차원의 output 생성
- 모델은 **head의 갯수(h) = 8**이고, Q, K, V의 차원은 h로 나눠줌으로써 계산량을 줄였다.(d<sub>k</sub>=d<sub>v</sub>=d<sub>model</sub>/h=64) 

```python
class Multi_head_attention(nn.Module):
    def __init__(self, d_model, n_head):
        super(Multi_head_attention, self).__init__()
        self.d_model = d_model
        self.n_head = n_head
        self.scaled_attention = Scaled_dot_product_attention()
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_cat = nn.Linear(d_model, d_model)
        self.layer_norm = nn.LayerNorm(d_model)

    def split_dim(self, batch_size, x):
        """[summary]
        Args:
            batch_size ([int]): 배치사이즈
            x ([tensor]): input tensor, x.shpae : batch, sequence, d_model

        Returns:
            [tensor]: batch_Size, n_head, sequence, d_model/n_head
        """
        out = x.view((batch_size,  -1,  self.n_head,  int(self.d_model/self.n_head)))   
        return out.transpose(1,2)     

    def __call__(self, q,k,v, mask=False):
        batch_size = q.size(0)
        query = self.split_dim(batch_size, self.w_q(q)) # batch_Size, n_head, sequence, d_model/n_head
        key = self.split_dim(batch_size, self.w_k(k)) # batch_Size, n_head, sequence, d_model/n_head
        value = self.split_dim(batch_size, self.w_v(v)) # batch_Size, n_head, sequence, d_model/n_head
        
        out = self.scaled_attention(query,key,value, mask=mask) # batch_Size, n_head, sequence, d_model/n_head
        out = out.transpose(1, 2) # batch_Size, sequence, n_head, d_model/n_head
        out = out.contiguous().view(batch_size, -1, self.d_model)  # batch_Size, sequence, d_model
        out = self.w_cat(out) # batch_Size, sequence, d_model
        return self.layer_norm((out+q))     # batch_Size, sequence, d_model
        
```

#### 3.2.3 Aplications of Attention in our Model
Transformer는 3가지 다른 방법으로 multi-head attention을 사용했다.
1. **encoder-decoder attention**
    - query는 이전 decoder layer, Key와 Value는 encoder의 출력에서 온다. 이는 input sequence의 모든 위치를 참조할 수 있게 한다.

2. encoder와 decoder에 포함된 **self-attetion layers**
    - 이전 인코더 레이어의 출력에서 query, key, value가 오기 때문에 이전 layer의 모든 위치를 참조할 수 있다.

3. decoder에 포함된 **Masked self-attention layer**
    - 디코더 출력은 다음 원소를 고려하지 않고 출력할 수 있게 마스킹함

```python    
def get_attention_pad_mask(query, key):
    batch_size, sequence = key.size()
    pad_mask = sequence==0
    pad_mask=pad_mask.unsqueeze(1).expand(batch_size, query, key)
    return pad_mask

def get_attn_subsequent_mask(seq):
    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]
    subsequent_mask = np.triu(np.ones(attn_shape), k=1)
    subsequent_mask = torch.from_numpy(subsequent_mask).byte()
    return subsequent_mask
```


### 3.3. Postion-wise Feed-Forward Networks
- attention layer에 외에 추가로 encoder, decoder의 각 레이어들은 추가로  Fully-connected Feed-Forward 네트워크를 갖고 있다.
<img src="/img/nlp1-5.jpg">
- Relu 활성함수와 2개의 linear 함수로 구성되어 있다.
- layer는 input 512, output 512, inner 2048의 차원을 가지고 있다.

```python
class Position_wise_feed_forward(nn.Module):
    def __init__(self, d_model, drop_out):
        super(Position_wise_feed_forward,self).__init__()
        self.fc_1=nn.Linear(d_model, 2048)
        self.fc_2=nn.Linear(2048, d_model)
        self.relu=nn.ReLU()
        self.drop_out=nn.Dropout(p= drop_out)
        self.layer_norm = nn.LayerNorm(d_model)
        
    def __call__(self, x):
        out = self.fc_2(self.drop_out(self.relu(self.fc_1(x)))) #  batch_Size, sequence, d_model
        return self.layer_norm((out+x))  #  batch_Size, sequence, d_model
        
```

### 3.4. Embedding and Softmax
- 다른 sequence 변환 모델들과 비슷하게 d<sub>model</sub>차원으로 학습된 임베딩을 사용하였다.
- 델에서는 2개의 embedding layer와 pre-softmax 선형변환 사이에 같은 weight 행렬을 사용했다. Embedding layer에는 
을 d<sub>k</sub><sup>1/2</sup>를 곱한다.

### 3.5. Positional Encoding
- 모델은 Recurrence와 convolution을 포함하고 있지 않기 때문에 sequence의 순서를 만들기 위해 상대적, 절대적 위치에 대한 정보를 투입해야 했다.
- 인코더와 디코더 stack 밑부분에 positional encodings를 추가하였다.
- positonal encoding의 차원도 d<sub>model</sub>이므로 기존 embedding layer에 더할 수 있다.
- **Positional Encoding**에 사용된 함수는 아래와 같다.
<p align="center" style="font-size:small"><img src="/img/nlp1-6.jpg">pos = position,    i = dimension</p>

- 사용 이유 : 길이가 다른 시퀀스에 대해서도 쉽게 encoding할 수 있기 때문
- `positional encoding은 향후 추가 posting 예정`

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_length):
        super(PositionalEncoding, self).__init__()
        position = torch.arange(0, max_length).unsqueeze(1)
        p_encoding = torch.zeros(max_length, d_model)
        p_encoding[:,0::2]= np.sin(position/np.power(10000, (np.arange(0, d_model, 2)/d_model)))
        p_encoding[:,1::2]= np.cos(position/np.power(10000, (np.arange(0, d_model, 2)/d_model)))
        p_encoding = p_encoding.unsqueeze(0).float()
        self.register_buffer('pe', p_encoding)
        
    def forward(self, x):
        return x + self.pe[:, :x.size(1),:]
        
```

## 4. Why Self-Attention
- symbol representations **x**(x<sub>1</sub>, ..., x<sub>n</sub>)를 continuous representations **z**(z<sub>1</sub>, ... , z<sub>n</sub>) 에 맵핑할 때, 다양한 조건에서 테스트해보았다.
<img src="/img/nlp1-7.jpg">

- 선택 이유
    1. Complexity per Layer즉, 레이어당 계산 복잡도가 제일 작다.
    2. Sequntial Operantion이 병렬처리로 인해 작다.
    3. long-range dependency를 잘 학습할 수 있다.(거리가 멀어도)

## 5. Explaination of Model Dimensions
- 가정 : korean_vocab_size(8000), english_vocab_size(30000), model 차원(d_model=512), num_encoder_layers(1), n_head(8), num_decoder_layers(1),  position_max_length(10000), batch(64), sequence_length(128)
1. Encoder
    1. input : 64, 128
    2. input Embedding : 64, 128, 512 (embeding vocab length = 8000)
    3. positional Embedding : 64, 128, 512 
    4. multi head attention : 64, 128, 512
        1. multi head attention input q, k, v = 64, 8, 128, 64
        2. multi_head_attention output : 64,128,512
    5. residual connection : 64, 128, 512
    6. feed forward : 64, 128, 512
    
2. Decoder
    1. input : 64, 128
    2. output Embedding : 64, 128, 512 (embeding vocab length = 30000)
    3. positional Embedding : 64, 128, 512 
    4. masked multi head attention : 64, 128, 512
    5. multi head attention : 64, 128, 512
    6. feed forward : 64, 128, 512
    7. linear : 64, 128, 30000

## 6. Example with visuallization
- 이미지 출처 : [허민석's transformer 강의 중 일부 발췌](www.youtube.com/watch?v=mxGCEWOxfe8&t=835s)
- I study at school → 난 학교에서 공부한다. 로 번역하는 과제 
<p align="center" style="font-size:small"><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbj1OhN%2FbtqY8Gv7InP%2FKviTGV5VS9DRhdp7i40pRk%2Fimg.png">Query, Key, Value 행렬 구하기</p>
<p align="center" style="font-size:small"><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fctb9DA%2FbtqZegb7yZ2%2F4jgKlOOsz9p9hCmRk5FEUK%2Fimg.png">Scaled Dot-product self-Attention</p>
<p align="center" style="font-size:small"><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbmdCRs%2FbtqZdlY01rw%2FVklyyYJs1o5KTZaKriJYh0%2Fimg.png">Multi-head Attention</p>
<p align="center" style="font-size:small"><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fburcqu%2FbtqZdlELyl5%2FeG0JTkQQK0i167BKjPhiI1%2Fimg.png">Feed-Forward</p>
<p align="center" style="font-size:small"><img src="/img/nlp1-8.jpg">result</p>