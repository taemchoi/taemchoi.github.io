---
title: "STS 프로젝트 2탄 : 'Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring'"
tags: [NLP, AI]
categories:
  - nlp
  - sts
date: 2022-03-24
toc : true
---

- [Abstract & Introduction & Related wor](#1-abstract--introduction--related-work)
- [Task](#2-task)
- [Methods](#3-methods)
  - [Transformers and Pre-Training Strategies](#31-transformers-and-pre-training-strategies)
  - [Bi-encoders](#32-bi-encoders)
  - [Cross-encoders](#33-cross-encoders)
  - [Poly-encoders](#34-poly-encoders)
- [Experiments](#4-experiments)


## 1. Abstract & Introduction & Related work
- 기존의 STS TASK에서 두 문장을 비교하는 방법론은 `CROSS-ENCODERS`와 `BI-ENCODERS`가 있었다.
    1. CROSS-ENCODERS
        - Context와 Candidate가 따로 구분되지 않고 이어 붙여 model의 input으로 활용
        - 장점 : global한 full sentence에 대해 self-attention을 진행하기 때문에 global한 특징을 추출할 수 있어 **성능(정확도)이 좋음**
        - 단점 : context와 candidate를 이어 붙여야하기 때문에 매번 인코딩을 재계산해줘야함 → **속도가 느림**

    2. BI-ENCODERS
        - context와 candidate를 각각 모델에 구분하여 태우고 마지막 result를 dot-product하여 score를 계산
        - 장점 : Context는 cash로 갖고 있고 candidate만 embedding하여 예측 시 2개의 representation을 활용하기에 **속도가 빠름**
        - 단점 : 각각 모델을 태우기 때문에 global한 특징을 배우지 못하여 **정확도가 떨어짐**

- Urbanek et al. (2019)에서 pre-trained된 bert를 활용하여 cross-encoder와 bi-encoder를 fine-tuning test를 진행함  
→ 그 결과, *cross-encoder가 정확도면에서 우수하나 computational cost가 많이 필요하여 느려서 실생활에 적용 불가능하다는 결론을 내림*

- `Poly-encoder`는 위에 설명한 두 인코더의 장점만을 모아 높은 정확도와 빠른 성능을 contribution으로 제시함  
→ Candidate의 embedding을 미리 계산해놓고 활용하여 속도를 줄임, Context embedding을 구할 때, Candidate와 attention하므로 global feature를 추출 가능함

## 2. Task
- Sentence selection in dialogue : 대화에서 다음에 올 적절한 문장 찾기
    - ConvAI2, DSTC7 challenge Track 1, Ubuntu V2 corpus

- Article Search in IR(Information Retrival) : 주어진 문장이 있는 Article Search
    - Wikipedia Article Search

- 평가는 각 후보에서 정답인 것을 선택(객관식)
<img src="/img/nlp/nlp3/0.jpg">

## 3. Methods
### 3.1. Transformers and Pre-Training Strategies
- Transformers
    - Architecture : Bert-base, 12-layers, 12-attention heads, 768-dim
    - pre-train : 2가지 준비  
        - "Wikipedia and the Toronto Books Corpus에서 input, label 쌍을 뽑음(150 million)" : BERT와 유사한 설정을 재현하면 이전에 보고된 것과 동일한 결과가 제공되는지 확인
        - "online platform Reddit에서 input, label 쌍을 뽑음(174 million) : 관심 있는 다운스트림task와 더 유사한 데이터에 대한 pre-train이 도움이 되는지 여부를 확인

- Input representation
    - Input과 label을 붙여서 pre-train에 활용, Input을 context label을 next sentence로 간주
    - special token [s] 활용

- Pre-training Procedure
    - Bert와 유사 (MLM, next sentence 구별, 학습동안 50%는 실제 next sentence 제공, 50%는 random sample)

- Fine-tuning
    - Cross-encoders, Bi-encoders, Poly-encoders 3가지로 진행

### 3.2. Bi-encoders
<img src="/img/nlp/nlp3/1.jpg">

- Input은 seperate되서 위 그림과 같이 들어감
- 수식 : **y<sub>ctxt</sub> = red(T<sub>1</sub>(ctxt)),   y<sub>cand</sub> = red(T<sub>2</sub>(cand))**
    - **T(x) = h<sub>1</sub> ... h<sub>N</sub>** : Transformer Encoder output
    - **red(·)** : sequence vector T를 one vector로 줄이는 function(average pooling 사용)
    - **y** : Contextualized Embedding, 하나의 문장 embedding의 최종 결과
    - **h<sub>1</sub> = [s]**

- Scoring : **s(ctxt, candi)**를 candidate score로 minimize crossentrophy 적용(s(ctxt, candi) = y<sub>ctxt</sub> ·y<sub>cand</sub>)
    - 1 batch = [(*ctxt<sub>1</sub>, cand<sub>1</sub>*), (*ctxt<sub>2</sub>, cand<sub>2</sub>*), (*ctxt<sub>n</sub>, cand<sub>n</sub>*)] 일 때, ctxt<sub>i</sub>와 cand<sub>1,...,n</sub>을 데이터로 활용하여 score가 가장 높은 것을 positive(1), 다른 것들은 negative(0)으로 one-hot vector를 구성하여 cross-entrophy를 구한다.

- Inference speed : cache를 활용하기 떄문에 속도가 빠름

### 3.3. Cross-encoders
<img src="/img/nlp/nlp3/2.jpg">

- Bert와 유사하게 두 문장을 연속된 문장으로 간주하고 이어 붙여서 하나의 single vector로 encoder에 들어감
- 수식 : y<sub>ctxt,cand</sub> = h<sub>1</sub> = f irst(T(ctxt, cand))
- context와 candidate가 함께 들어가기 때문에 둘 사이의 self-attention을 적용하여 정확도가 더 높아지는 장점이 있음
- 마지막에 fc-layer로 scala값으로 줄여 score를 구하게 됨
- scoring은 Bi-encoder와 유사

### 3.4. Poly-encoders
<img src="/img/nlp/nlp3/3.jpg">

- Bi-encoder와 같이 2개의 transformer를 사용하여 별도로 인코딩 진행
- Context-Encoder Emb<sub>1...m</sub> 수식
    - y<sup>i</sup><sub>ctxt</sub> = ∑<sub>j</sub>w<sup>c<sub>i</sub></sup><sub>j</sub>h<sub>j</sub>,   where(w<sup>c<sub>i</sub></sup><sub>1</sub>,...,w<sup>c<sub>i</sub></sup><sub>N</sub>)=softmax(c<sub>i</sub>⋅h<sub>1</sub>,...,c<sub>i</sub>⋅h<sub>N</sub>)
    - code vector *m*(random initialized)을 query로 하여 인코더의 결과에 각 attention을 진행하여 context vector의 중요한 의미를 강조 추출한다.  
    → m은 learnable parameter이다.
- 최종 Ctxt Emb 수식
    - y<sub>ctxt</sub> = ∑<sub>i</sub>w<sub>i</sub>y<sup>i</sup><sub>ctxt</sub>,   where(w<sub>1</sub>,...,w<sub>m</sub>)=softmax(y<sub>cand<sub>i</sub></sub>·y<sub>1<sub>ctxt</sub></sub>,...,y<sub>cand<sub>i</sub></sub>·y<sub>m<sub>ctxt</sub></sub>)
    - cand encoder에서 구한 embedding과 attention을 통해 최종 ctxt embedding vector를 구한다.
- 마지막으로 ctxt emb와 cand emb의 dot-product를 통해 bi-encoder와 동일하게 score를 구한다.


## 4. Experiments
<img src="/img/nlp/nlp3/4.jpg">

- 정확도 : `Bi-encoders` < `Poly-encoders` < `Cross-encoders`
- 속도 : `Cross-encoders` < `Poly-encoders` < `Bi-encoders`  
Cross-encoders는 속도가 너무 떨어져서 실 서비스에는 사용불가

## 5. reference
<a href="https://arxiv.org/abs/1905.01969">poly-encoder논문</a>