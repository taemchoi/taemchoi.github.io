---
title: "STS 프로젝트 3탄 : 'Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks'"
tags: [NLP, AI]
categories:
  - nlp
  - sts
date: 2022-03-27
toc : true
---

- [Abstract & Introduction & Related work](#1-abstract--introduction--related-work)
  - [Abstract](#11-abstract)
  - [Introduction](#12-introduction)
  - [Related Work](#13-related-work)
- [Methods](#2-methods)
  - [Augmented SBERT](#21-augmented-sbert)
  - [Sampling Strategies](#22-sampling-strategies)
  - [Domain Adaptation with AugSBERT](#23-domain-adaptation-with-augsbert)
- [Datasets](#3-datasets)
- [Experimental Setup](#4-experimental-setup)
- [Result and Discussion](#5-result-and-discussion)




## 1. Abstract & Introduction & Related work
### 1.1 Abstract
- pairwise sentence scoring에는 Cross-encoder와 Bi-encoder 방식이 있다.
- cross-encoder는 성능은 좋으나 너무 느리고 bi-encoder는 속도는 비교적 빠르나 같은 도메인으로 fine-tune해야 성능을 기대할 수 있다.
- 간단하면서 효과적인 데이터 augmentation 전략을 가진 **Augmented SBERT**를 제시한다.  
→ 간단히 말하자면, cross-encoder로 라벨링하고 bi-encoder로 학습한다. 즉, 같은 도메인의 데이터는 annotated 된 데이터가 없기 때문에 이를 맞춰주는 작업이다.

- 평가도 in-domain 외에 out-domain에서도 진행하여 측정하였고, SBERT 대비 좋은 성능을 가져올 수 있었다.

### 1.2. Introduction
- pairwise sentence scoring은 *IR, QA, duplicate question detection, clustering 등*에서 활용된다.
- cross-encoder는 시간이 오래 걸리고(<a herf="https://taemchoi.github.io/nlp/sts/nlp-3/">poly-encoder</a> 포스트에서 충분한 설명) 색인화할 수 있는 입력에 독립적으로 representation을 산출하지 않기 때문에 IR 등에서 활용이 불가능하다.(input이 항상 2개가 묶여서 들어오기 떄문)
- Bi-encoder(SBERT)는 독립적으로 각 문장을 벡터 밀집공간에 mapping 할 수 있다. → 효과적으로 indexing과 비교를 할 수 있음.  
→ 부가설명 : 단일 query를 임베딩하고 벡터공간 내에 유사도 top1을 뽑을 수 있음을 의미함
- SBERT는 BERT(cross-encoder)에 비해 낮은 성능을 보여주는 단점이 있다.
- SBERT의 모델 설명은 뒤에서 예정(일종의 cross-encoder와 bi-encoder의 앙상블로도 볼 수 있음)
- Argument similarity, semantic textual similarity, duplicate question detection, news paraphrase identification 4가지 task에서 평가를 진행하였다.

### 1.3. Related Work
- 설명은 하지 않겠지만, 이전 연구에는 Skip-thought, InferSent, USE가 있다.(<a herf="https://taemchoi.github.io/nlp/sts/nlp-1/">SBERT</a> 포스트에서 약간 설명)
- <a href="https://taem-learning.tistory.com/25?category=1007464">BERT</a>, <a herf="https://taemchoi.github.io/nlp/sts/nlp-3/">poly-encoder</a>, DiPair(추론 속도에 포커싱하고 bi-encoder의 속도와 성능간의 trade-off 관계의 최적화에 대한 실험을 진행함)
- 우리의 모델은 일종의 semi-supervision 기반이다. 우리는 suitable senetence pair를 생성이 필요하고 이를 위해 다양한 sampling을 시도하였다.(method에서 설명)

## 2. Methods
### 2.1. Augmented SBERT
<img src="/img/nlp/nlp4/0.jpg">

- 우수한 성능으로 사전학습된 Cross-encoder를 사용해서 sampling 전략에 의해 sampling된 sentence pair를 labeling 한다.  
→ 실버 데이터셋
- 이 실버데이터셋과 골드 데이터셋이 병합하고 Bi-encoder(SBERT)를 fine-tuning한다.
 
### 2.2. Sampling Strategies
- Pair Sampling Strategies
    - 새로운 sentence pair는 새로운 데이터이거나 Gold dataset 독립적 문장을 재사용해서 pair를 재결하여 만들 수 있다.
    - 그러나, Cross-encoder 연산은 <sub>n</sub>C<sub>2</sub>여서 overhead 가능성이 있기 때문에 sampling해서 데이터를 줄이고 성능을 올리는 전략이 중요하다.
1. **Random Sampling**
    - negative로 skew된 라벨 분포가 생성될 수 있기 때문에 주의해야한다.

2. **Kernel Density Estimation (KDE)**
    - classification에서는 positive pair는 남기고 negative pair는 sampling하면 된다.
    - 반면에, Regression에서는 KDE를 활용하여 score s에 대한 연속확률밀도 함수 F<sub>gold</sub>(s)와 F<sub>silver</sub>(s)를 추정한다.  
    그리고, KL-Divergence를 활용해서 두 분포간의 거리를 최소화하도록 훈련한다.  
    이 함수를 이용해서 score probability Q(s)를 구할 수 있다.
<img src="/img/nlp/nlp4/1.jpg">

3. **BM25 Sampling**
    - 어휘 중복 기반의 알고리즘으로 흔히 많은 검색 엔진의 scoring function으로 활용
    - 연구에서는 ElasticSearch(빠른 검색을 도와주는 색인어를 생성)에 내장된 BM25알고리즘을 활용하여 indexing을 하고 유사도를 측정하여 top similar pair를
 선정하고 silver dataset으로 활용한다.

4. **Semantic Search Sampling(SS)**
    - BM25의 결점은 어휘중복 기반이기 때문에 동의어나 약간의 어휘 중복은 못 찾는 단점이 있다.(이런 경우, silver dataset은 없음)
    - 우리는 goldset을 활용하여 SBERT를 훈련하고 이를 similar sentence pair를 추가로 샘플링하는데 활용한다.
    - 사전 훈련된 SBERT는 collection에서 가장 유사한 상위 k 개의 문장을 검색하는데 사용합니다. Large collection의 경우는 Faiss와 같은 최대 근사 이웃 검색을 사용하여 k개의 가장 비슷한 문장을 빠르게 검색할 수 있다.

5. **BM25 + SS**
    - 두 장점을 활용하지만 때때로, skew된 분포가 만들어질 수 있다.

6. **Seed Optimization**
    - 5개의 random seed로 훈련하고 최고 모델을 선정한다.(+early stopping 20%)

### 2.3. Domain Adaptation with AugSBERT
- 도메인이 다를 경우, 나타지 않았던 문장을 벡터공간에 정확히 mapping 할 수 없기 떄문에 성능 저하가 발생한다.
- 도메인이 다를 경우, 학습 전략
    - 1. cross-encoder를 domain 전체로 fine-tune 한다.(label되어 있음)
    - 2. target domain을 이 cross-encoder를 이용하여 labeling한다.
    - 3. label된 데이터로 bi-encoder(SBERT)를 학습한다.
 <img src="/img/nlp/nlp4/2.jpg">

## 3. Datasets
### 3.1. Single-Domain Datasets(pre-train 도메인과 target-domain이 같은 경우)
- Regression은 semantic textual similarity와 argument similarity, Binary sentence pair classification은 Duplicate question detection 과 news paraphrase identification을 task로 사용하였다.
- SemEval Spanish STS : STS는 두 문장간 유사한 정도를 보는 task이다.(0~5점)
- BWS Argument Similarity Dataset (BWS) : 기존 데이터 세트에는 sentence pair 선택/샘플링 프로세스 항상 이해할 수 있는 것은 아니라는 단점이 있다.
이를 극복하기 위해 argument 유사성을 위한 새로운 셋을 만들었다.
- Quora Question Pairs(Quora-QP) : 중복된 의미의 질문을 찾는 task이다.(404,290개 pair로 구성)
- Microsoft Research Paraphrase Corpus : 온라인 뉴스에서 뽑은 sentence pair로 구성되어 있는 데이터넷의 유사성을 식별하는 task
 
<img src="/img/nlp/nlp4/3.jpg">

### 3.2. Multi-Domain Datasets(pre-train 도메인과 target-domain이 다른 경우)
- question-question pair의 binary classification에 초점
- AskUbuntu, Quora, Sprint, SuperUser dataset으로 test 진행

# 4. Experimental Setup
<img src="/img/nlp/nlp4/4.jpg">

# 5. Result and Discussion
<img src="/img/nlp/nlp4/5.jpg">

- in-domain에서 학습된 Bi-encoder보다 cross-domain에서 학습한 AugSBERT가 좋은 성능을 내고 있는 것을 확인할 수 있다.