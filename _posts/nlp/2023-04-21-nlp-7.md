---
title: "SELF-INSTRUCT - Aligning Language Model with Self Generated Instructions" 
tags: [NLP, AI]
categories:
  - nlp
  - llm
date: 2023-04-21
toc : true
---

[SELF-INSTRUCT 논문 원본](https://arxiv.org/pdf/2212.10560.pdf)

### 1. Abstract

- 양과 다양성, 창의성이 제한된 사람이 작성한 명령어 데이터에 크게 의존하기 때문에 튜닝된 모델의 일반화를 제한하고 있다.
- 여기서 제시한 모델 `SELF-INSTRUCT` 설명은 뒤에서 다시 다루겠다.
- SELF-INSTRUCT는 사람이 개입한 InstructGPT에 비해 약 5%의 절대적인 차이만 존재했다.
    - cost는 낮은데 성능은 충분히 높음을 설명
- SELF-INSTRUCT는 사전 학습된 언어 모델과 Instruction을 aligning 하는데 거의 annotation이 필요없는 방법을 제공
- 미래의 Instruction - tuning에 대한 연구를 촉진하기 위해 대규모 합성 데이터셋도 함께 공개함

### 2. Introduction

- 최근 NLP 문헌에서는 자연어 지시를 따를 수 있는 거대 모델을 만드는 활동이 이루어지고 있음
- PROMPT SOURCE와 SUPER NATURAL INSTRUCTIONS은 T0모델과 Tk-INSTRUCT을 구축하기 위한 Instruction을 수집하귀 위한 넒은 manual anotation을 사용하는 최근 주목할만한 데이터셋이다.
    - human의 개입으로 인한 cost가 너무 많이 들고 앞서 언급한 다양성 등 문제가 한계가 존재함
- `SELF-INSTRUCT` 는 모델 자신의 instructional signal을 사용하는 instruction-tuning을 위한 반자동 프로세스이다.
- 모델의 구동에 대한 내용이 있지만, 해당 내용은 method에서 설명
- Contribution
    - 최소한의 라벨링으로 instruction을 따르도록 유도하는 방법이다.
    - 다양한 실험을 통해 효과를 입증함
    - 52K의 instruction 데이터셋과 수동으로 작업한 새로운 테스크의 데이터셋을 함께 공개함
    
    <img src="/img/nlp/nlp7/0.jpg">    

### 3. Method

- 하기의 사유로 large scale의 instruction 데이터는 사람에게 어려움
    - 새로운 task에 대한 창의성 필요
    - 전문성 필요
- `Step 0. Defining Instruction Data`
    - **우리가 생성하기 원하는 instruction 데이터는 instruction(task t로 정의)의 집합이다. {$I_t$}**
    - 각각의 task t는 하나 혹은 그이상의 input-output instance를 갖는다. $(X_t, Y_t)$
    - Model을 $M$, output $y$라 할 때, 다음의 정의를 따른다.
    - $M(I_t, x) = y, for (x,y) \subseteq (X_t, Y_t)$
    - 다양성을 위해 input $x$가 없는 instruction도 허용한다.
    - **Instruction** - “write an essay about school safety”, **Input** 없음
    - **Instruction** - “write an essay about the following topic”, **Input** - “school safety”
- `step 1. Automatic Instruction Data Generation`
    - 4단계로 instruction 데이터를 생성함
    1. Instruction Generaion
        - `SELF-INSTRUCT`는 Context에서 존재하는 instruction들을 보여줬을 때, 새로운 instruction들을 생성하기 위한 prompt를 할 수 있는 사전 학습된 LLM을 찾는 것을 기본으로 한다.
        - Task pool을 1개의 instruction과 각 task를 위한 1개의 instance를 가진175개의 task들로 초기화한다.
        - 매 step마다, 8개의 task instruction을 sampling한다.(사람 6, 모델 2) → in-context learning의 개념
            
            <img src="/img/nlp/nlp7/1.jpg">
            
    2.  Classification Task Identification
        - 성능을 위해 classification인지 아닌지를 구분함
        - 12개의 classification과 19개의 non-classification을 few-shot으로 줌
            
            <img src="/img/nlp/nlp7/2.jpg">
            
    3. Instance Generation
        - Instruction과 task type이 주어졌을 때, 각각의 instruction마다 instance를 생성함
        - 이는 모델이 instruction 기반으로 목표작업이 뭔제 이해하고, 추가 입력 필드가 필요한지 파악하여 생성하고,  마지막으로 출력을 생성해야 하기 때문에 어려운 일임
        - 다른 task의 입출력 예시를 제시할 때, 이를 일부분 해결할 수 있음을 알게됨
        - Input-First Approach : 언어 모델에 instruction을 기반으로 input field를 먼저 나타낸 다음 output을 생성하도록 유도
            
            <img src="/img/nlp/nlp7/3.jpg">
            
        - Output-First Approach : 분류 작업을 위해 가능한 클래스 레이블을 먼저 생성한 다음 각 클래스 레이블에 대한 입력 생성을 조건부로 생성하도록 유도
            
            <img src="/img/nlp/nlp7/4.jpg">
            
    4. Filtering and Postprocessing
        - 다양성을 위해, ROUGE-L overlap이 0.7 미만인 경우에만 task pool에 추가
        - 언어모델이 처리할 수 없는 이미지, 음성 등의 키워드 제거
        - 각 instruction은 새 instance를 생성할 때, 완전히 동일한 instnace나 입력이 같지만 출력이 다른 instance는 필터링함
- `Step 3. Finetuning the LM to Follow Instructions`
    - 이제 생성된 데이터들을 사용하여 supervised 방식으로 original LM을 fine-tuning함
    - 다른 포멧에 robust한 모델을 만들기 위해 여러 템플릿을 사용하여 instruction과 instance input을 함께 인코딩 함
    - 예르들어 “Task:”, “Input:”, “Output:” 등을 붙이거나 안붙이거나 줄바꿈을 넣는 등의 트릭을 넣는 거임

### 4. SELF-INSTRUCT Data From GPT3

- Statistics
    - We generate a total of over 52K instructions, and more than 82K instances corresponding to these instructions after filtering.
        
        <img src="/img/nlp/nlp7/5.jpg">
        

### 5. Experiments ans Results

- InstructGPT에 크게 뒤쳐지지 않음

<img src="/img/nlp/nlp7/6.jpg">

<img src="/img/nlp/nlp7/7.jpg">

### 6. LIMITATION

- Tail phenomena : LM에 의존하며, LM이 가지는 한계를 상속하고 있다. 즉, LM을 기반으로 데이터셋을 취득하는 것이기 때문에 창의성 측면에서는 취약하다.
- Dependence on large Models : 대규모 모델에 의존하기 때문에 진입장벽이 있다.
- Reinforcing LM biases : 의도하지 않은 결과를 우려함, 반복 알고리즘은 문제가 되는 사회적 편견의 증폭과 같은 문제를 일으킨 수 있음