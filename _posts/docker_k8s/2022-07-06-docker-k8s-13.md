---
title: "k8s - Lifecycle(Status, ReadinessProbe, LivenessProbe, QoS Classes, Node Scheduling)"
tags: [k8s]
categories:
  - docker-k8s
date: 2022-07-06

toc : true
---

## Pod의 Lifecycle 
### Pod Statues
- pod를 생성하면 나오는 상태정보
#### 1. Phase
- Pending : 파드의 최초 상태, 
- Running : 컨테이너가 최초 생성이 완료되면 Running으로 바뀜(내부 컨테이너가 돌아가지 않을 경우에도 Running이다.)
- Succeeded : Job으로 돌 경우, 정상적으로 완료되면 succeeded
- Failed : Job으로 돌 경우, 실패하면 Failed, pending중에 Failed가 되기도 함
- Unknown : network장애가 생기면 unknown으로 됨
  
#### 2. Conditions
- Type
    - Initialized : Pending상태일 경우, initContainer 옵션이 있을 경우, 이를 성공할 경우 True, 실패할 경우 False 
    - PodScheduled : 어느 노드에 Pod를 생성할지 정하는 것, 이게 끝나야 initalized가 진행 됨
    - ContainerReady : 파드가 Running이어도 컨테이너는 작동중이지 않을 수 있기 때문에 이를 확인해야함. 컨테이너가 돌아야 True
    - Ready : 파드가 Running이어도 컨테이너는 작동중이지 않을 수 있기 때문에 이를 확인해야함. 컨테이너가 돌아야 True(일반적으로 서비스가 유지되어야할 경우, 항상 모니터링 해줘야함)

- Reason : Status가 False일 경우, 이유를 설명
    - ContainersNotReady : 컨테이너 쪽 작업이 진행중이다.
    - PodCompleted  

#### 3. containerStatuses
- State 
    - Wating : 이미지 다운로드 중일 경우, True
    - Running : 정상 작동 중
    - Terminated : 종료

- Reason
    - ContainerCreating : 이미지 다운로드 중일 경우, True
    - CrashLoopBackOff : 컨테이너가 문제가 생겨서 재시작 될 경우 
    - Error 
    - Completed

### ReadnessProbe와 LivenessProbe
- 둘다 최초 pod를 생성할 때, app을 모니터링하여 안정적으로 운영하기 위해 사용된다.
- 예시 : 한 서비스에 두 개의 노드의 각 파드로 서비스를 하고 있다고 한다면 Traffic은 50%씩 나눠서 서비스되고 있을 것이다.
그런데 노드 하나가 죽으면 다른 노드에 100% 트래픽이 걸리고 이 트래픽을 기존 노드가 견디는 동안 새로운 노드에 파드를 만들게 된다.
파드가 러닝 상태가 되면 app이 구동됬는지와 상관없이 트래픽이 걸리게 되고 그러면 오류가 발생하게 되니 ReadinessProbe를 걸어 놓으면 app이 완전히 구동되야 트래픽이 분산되게 된다.
그런데 이 app이 오류에 의해 죽게되면 파드는 러닝이므로 트래픽 에러는 다시 발생하게 되는데 이럴 경우, 지속되는 오류를 방지하기 위해 앱을 감지하여 파드를 재시작시켜주도록 하는 것이 livenessProbe이다.

#### ReadnessProbe, LivenessProbe 옵션
컨테이너가 러닝상태가 되면 시작되게 됨
- httpGet : port, host, path, httpHeader, Scheme 으로 app을 체크
- Exec : 특정 명령어를 날려서 그에 따른 결과 체크
- tcpSocket : port, host를 확인하여 app체크
- initialDelaySeconds : 최초 대기 시간
- periodSeconds : 시간 간격으로 체크
- timeoutSeconds : 몇 초내로 결과가 안날아오면 failed로 알지
- successThreshold : 몇 번 성공해야 성공으로 간주할지
- failureThreshold : 몇 번 실패해야 실패로 간주할지

<img src="/img/docker_k8s/13/4.jpg">

```yml
# 1-1
apiVersion: v1
kind: Service
metadata:
  name: svc-readiness
spec:
  selector:
    app: readiness
  ports:
  - port: 8080
    targetPort: 8080

# 1-2
apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    app: readiness  
spec:
  containers:
  - name: container
    image: kubetm/app
    ports:
    - containerPort: 8080	
  terminationGracePeriodSeconds: 0

# 1-3
apiVersion: v1
kind: Pod
metadata:
  name: pod-readiness-exec1
  labels:
    app: readiness  
spec:
  containers:
  - name: readiness
    image: kubetm/app
    ports:
    - containerPort: 8080	
    readinessProbe:
      exec:
        command: ["cat", "/readiness/ready.txt"]
      initialDelaySeconds: 5
      periodSeconds: 10
      successThreshold: 3
    volumeMounts:
    - name: host-path
      mountPath: /readiness
  volumes:
  - name : host-path
    hostPath:
      path: /tmp/readiness
      type: DirectoryOrCreate
  terminationGracePeriodSeconds: 0
```

<img src="/img/docker_k8s/13/5.jpg">

```yml
# 2-1
apiVersion: v1
kind: Service
metadata:
  name: svc-liveness
spec:
  selector:
    app: liveness
  ports:
  - port: 8080
    targetPort: 8080

# 2-2 
apiVersion: v1
kind: Pod
metadata:
  name: pod2
  labels:
    app: liveness
spec:
  containers:
  - name: container
    image: kubetm/app
    ports:
    - containerPort: 8080
  terminationGracePeriodSeconds: 0

# 2-3
apiVersion: v1
kind: Pod
metadata:
  name: pod-liveness-httpget1
  labels:
    app: liveness
spec:
  containers:
  - name: liveness
    image: kubetm/app
    ports:
    - containerPort: 8080
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 10
      failureThreshold: 3
  terminationGracePeriodSeconds: 0
```

### Qos classes(Quality of Service)
- 노드에 파드가 3개 있고 균등하게 자원을 사용 중이다. 근데 파드 1에 추가 리소스가 필요하다면 앱의 중요도에 따라 자원을 당겨오게 된다.
- 가장 먼저 BestEffort가 다운되고 Burstable이 다음에 다운되고 마지막에 Guaranteed가 다운되게 된다.
    - BestEffort : Pod에 어떤 컨테이너 내에도 request와 limit이 설정되어 있지 않은 경우
    - Burstable : Pod에 컨테이너들이 모두 request와 limit은 설정되어 있으나 두 개의 값이 같지 않은 경우(Burstable 내부에서 삭제 순위는 OOM스코어를 보고 파드를 먼저 제거하게 되는데 가용률이 높은 파드부터 삭제가 됨)
    - Guaranteed : Pod에 컨테이너들이 모두 request와 limit가 설정되어 있고 컨테이너 내의 request와 limit값이 서로 같은 경우
- 컨테이너의 resource의 request와 limit에 따라 자동으로 설정됨

### Node Scheduling(Pod)
#### Pod를 특정 노드에 할당되도록 선택하는 용도
- NodeName : 스케줄러에 상관없이 노드가 절대값으로 할당됨
    - 명시적 할당으로 좋아보이지만, 실제 사용환경에서는 노드이름이 계속 변경되므로 잘 사용안함
- NodeSelector : NodeName 대신 Key:value 구조로 노드의 label을 보고 할당이 됨
    - 여러 노드에 같은 keyvalue가 있을 경우, 자원이 많은 노드로 할당됨
    - key,value 매칭이 되는 노드가 없으면 에러가 남)
- NodeAffinity : key만으로 할당가능
    - <img src="/img/docker_k8s/13/0.jpg">
    - 매칭 노드가 없을 시, 자원이 풍부한 노드에 자동으로 할당됨
    - required VS preferred : required 옵션(key가 매칭되지 않으면 절대 할당되지 않음), preferred 옵션(key가 매칭되지 않아도 유연하게 할당됨)
    - preferred weight : pod의 키가 2개일 경우, weight를 두고 단순 cpu가용률 외에 weight를 가지고 score를 계산하여 할당하게 됨
```yml
apiVersion: v1
kind: Pod
metadata:
 name: pod-required
spec:
 affinity:
  nodeAffinity:
   requiredDuringSchedulingIgnoredDuringExecution:
    nodeSelectorTerms:
    - matchExpressions:
      - {key: ch, operator: Exists}
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0

 apiVersion: v1
kind: Pod
metadata:
 name: pod-preferred
spec:
 affinity:
  nodeAffinity:
   preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 1
      preference:
       matchExpressions:
       - {key: ch, operator: Exists}
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0
```

#### Pod를 집중해서 할당하거나 겹치지않게 할당
- Pod Affinity : 한 노드에 집중해서 할당
    - <img src="/img/docker_k8s/13/1.jpg">
    - web 파드(type:web)가 스케줄러에 의해 노드에 할당하고 같은 노드에 server 노드도 할당하려면 파드에 PodAffinity 속성을 놓고 Type:web으로 지정하면 같은 노드에 할당되게 됨
    - topologyKey : Node의 키와 매칭시킴(할당되는 범위를 줄여줌)
```yml
# 2-1
kubectl label nodes k8s-node1 a-team=1
kubectl label nodes k8s-node2 a-team=2

# 2-2
apiVersion: v1
kind: Pod
metadata:
 name: web1
 labels:
  type: web1
spec:
 nodeSelector:
  a-team: '1'
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0

# 2-3 
apiVersion: v1
kind: Pod
metadata:
 name: server1
spec:
 affinity:
  podAffinity:
   requiredDuringSchedulingIgnoredDuringExecution:   
   - topologyKey: a-team
     labelSelector:
      matchExpressions:
      -  {key: type, operator: In, values: [web1]}
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0

 # 2-4
 apiVersion: v1
kind: Pod
metadata:
 name: server2
spec:
 affinity:
  podAffinity:
   requiredDuringSchedulingIgnoredDuringExecution:   
   - topologyKey: a-team
     labelSelector:
      matchExpressions:
      -  {key: type, operator: In, values: [web2]}
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0
 
 # 2-5
 apiVersion: v1
kind: Pod
metadata:
  name: web2
  labels:
     type: web2
spec:
  nodeSelector:
    a-team: '2'
  containers:
  - name: container
    image: kubetm/app
  terminationGracePeriodSeconds: 0
```


- Anti-Affinity : 겹치지않게 할당
    - <img src="/img/docker_k8s/13/2.jpg">
    - Master가 죽으면 Slave가 백업을 해줘야 하는 경우, 서로 다른 노드에 있어야 함
    - Master가 노드에 할당되고 Slave에 Anti-Affinity를 Type:Master로 주면 다른 노드에 할당되게 됨
    - topologyKey : Node의 키와 매칭시킴(할당되는 범위를 줄여줌)
```yml
# 2-6
apiVersion: v1
kind: Pod
metadata:
  name: master
  labels:
     type: master
spec:
  nodeSelector:
    a-team: '1'
  containers:
  - name: container
    image: kubetm/app
  terminationGracePeriodSeconds: 0

# 2-7
apiVersion: v1
kind: Pod
metadata:
 name: slave
spec:
 affinity:
  podAntiAffinity:
   requiredDuringSchedulingIgnoredDuringExecution:   
   - topologyKey: a-team
     labelSelector:
      matchExpressions:
      -  {key: type, operator: In, values: [master]}
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0
```


#### 특정 Node는 아무 파드나 할당되지 않도록 제한
- Traint : 할당 제한을 둠(Toleration 필요)
    - <img src="/img/docker_k8s/13/3.jpg">
    - 노드 스케줄러로도 할당이 안되고 NodeName을 명시해도 할당이 안됨
    - 할당을 하고 싶으면 Toleration을 달고 와야함
    - pod가 해당 Node로 스케줄링되려면 Toleration 외에도 nodeSelector로 스케줄링을 해줘야 함
    - NoSchedule : pod1이 node2에 운영중인데 node2에 NoSchedule 옵션을 준 taint를 단다고 해서 pod가 삭제되지 않음(최초 진입에서만 필터링)
    - NoExecute : pod2가 node3에 할당되어 있을 때, NoExecute taint를 달면 pod2는 삭제가 됨. 삭제 안되려면 생성시에 NoExecute을 달아놔야 함(tolerationSeconds은 해당 시간 후에 pod가 삭제되도록 하는 옵션)

```yml
# 3-1
kubectl label nodes k8s-node1 gpu=no1
# 3-2
kubectl taint nodes k8s-node1 hw=gpu:NoSchedule
# 3-3
apiVersion: v1
kind: Pod
metadata:
 name: pod-with-toleration
spec:
 nodeSelector:
  gpu: no1
 tolerations:
 - effect: NoSchedule
   key: hw
   operator: Equal
   value: gpu
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0
# 3-4
apiVersion: v1
kind: Pod
metadata:
 name: pod-without-toleration
spec:
 nodeSelector:
  gpu: no1
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0

# 3-5
apiVersion: v1
kind: Pod
metadata:
 name: pod1-with-no-execute
spec:
 tolerations:
 - effect: NoExecute
   key: out-of-disk
   operator: Exists
   tolerationSeconds: 30
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0

# 3-6
apiVersion: v1
kind: Pod
metadata:
 name: pod2-without-no-execute
spec:
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0

 # 3-7
kubectl taint nodes k8s-node1 hw=gpu:NoSchedule-
kubectl taint nodes k8s-node2 out-of-disk=True:NoExecute
```

### 출처
- 이미지 : https://kubetm.github.io/k8s/06-intermediate-pod/nodescheduling/
- 강의 : `대세는 쿠버네티스 [초급~중급](인프런)`
